{
  "model_name": "mlp-other-rate/trial_33_a82110",
  "model_type": "MLP",
  "model_format": "torch",
  "model_params": {
    "history_length": 1,
    "hidden_layers": [
      512,
      256,
      128
    ],
    "dropout": 0.3934723904888991,
    "mid_layer_count": 13,
    "mid_layer_size": 155
  },
  "training_params": {
    "max_epochs": 400,
    "batch_size": 204,
    "learning_rate": 0.001224204949895237,
    "weight_decay": 0.0003562882021971082,
    "checkpoint_interval": 10,
    "seed": 42
  },
  "data_csv": "data/time_aligned_data.csv",
  "timestamp_column": "Date, Time",
  "columns": [
    "TRC-DT",
    "TRC-RT",
    "TRC-PPL1",
    "TRC-PPL2",
    "pH-DT",
    "pH-RT",
    "pH-PPL1",
    "pH-PPL2",
    "cond-DT",
    "cond-PPL1",
    "cond-PPL2",
    "fDOM-RT",
    "fDOM-PPL1",
    "fDOM-PPL2",
    "DO-RT",
    "DO-PPL1",
    "DO-PPL2",
    "TOC-RT",
    "TOC-PPL1",
    "TOC-PPL2",
    "DOC-RT",
    "DOC-PPL1",
    "DOC-PPL2",
    "minutes_since_start"
  ],
  "feature_columns": [
    "TRC-DT",
    "TRC-RT",
    "pH-DT",
    "pH-RT",
    "cond-DT",
    "fDOM-RT",
    "DO-RT",
    "TOC-RT",
    "DOC-RT",
    "minutes_since_start"
  ],
  "target_columns": [
    "TOC-PPL1",
    "TOC-PPL2",
    "DOC-PPL1",
    "DOC-PPL2",
    "pH-PPL1",
    "pH-PPL2"
  ],
  "target_mode": "rate",
  "target_base_columns": [
    "TOC-RT",
    "TOC-RT",
    "DOC-RT",
    "DOC-RT",
    "pH-RT",
    "pH-RT"
  ],
  "split_boundaries": {
    "train_end": 7796,
    "val_end": 9466,
    "test_end": 11138
  },
  "dataset_sizes": {
    "train": 7796,
    "val": 1670,
    "test": 1672
  },
  "input_dim": 10,
  "sequence_length": null,
  "training_history": {
    "epochs": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46,
      47,
      48,
      49,
      50,
      51,
      52,
      53,
      54,
      55,
      56,
      57,
      58,
      59,
      60,
      61,
      62,
      63,
      64,
      65,
      66,
      67,
      68,
      69,
      70,
      71,
      72,
      73,
      74,
      75,
      76,
      77,
      78,
      79,
      80,
      81,
      82,
      83,
      84,
      85,
      86,
      87,
      88,
      89,
      90,
      91,
      92,
      93,
      94,
      95,
      96,
      97,
      98,
      99,
      100,
      101,
      102,
      103,
      104,
      105,
      106,
      107,
      108,
      109,
      110,
      111,
      112,
      113,
      114,
      115,
      116,
      117,
      118,
      119,
      120,
      121,
      122,
      123,
      124,
      125,
      126,
      127,
      128,
      129
    ],
    "train_loss": [
      0.4240759349499316,
      0.2776714101349653,
      0.2535816899946618,
      0.23542083080393525,
      0.22327027742436997,
      0.21688552063174465,
      0.2082252249756246,
      0.19918119151999608,
      0.19472537664127937,
      0.18951414867264604,
      0.18647071096057952,
      0.1810157875056876,
      0.17976403558266474,
      0.17482706534367087,
      0.1747523331379144,
      0.1677701254726496,
      0.16693808446485853,
      0.16182135605946635,
      0.1617831771131538,
      0.15897505453239655,
      0.1582678064375183,
      0.1560208743095031,
      0.15258195930532825,
      0.15542663353232006,
      0.14896985113987743,
      0.1496237333751202,
      0.1482684625424502,
      0.14639348346738096,
      0.1439985410220015,
      0.14156332252697187,
      0.14358507587228939,
      0.1417254735837538,
      0.13979913490041213,
      0.13945805600984332,
      0.13777306402582826,
      0.13819578181418962,
      0.13594646615407724,
      0.1357893891414659,
      0.13485783628312792,
      0.13458509979078623,
      0.13228795643400815,
      0.13372000471988416,
      0.13265278519132676,
      0.13072686415335777,
      0.13077438549315887,
      0.1302354306578942,
      0.12857027461952036,
      0.1299608724393436,
      0.12789284536904588,
      0.12716337699364735,
      0.12748714967254737,
      0.12753664957155872,
      0.12557480413908223,
      0.1271589239127456,
      0.1249468227253014,
      0.12319128744384458,
      0.12449929409833857,
      0.12450033724078648,
      0.12311743494178651,
      0.12265236558762498,
      0.12070026076286496,
      0.12144660784945603,
      0.12219993549166366,
      0.12175372971547696,
      0.12106673394627544,
      0.12265234307150402,
      0.1199304579411671,
      0.12019717401269156,
      0.12116650697235697,
      0.11910326371763595,
      0.11656900950965055,
      0.11883997870155211,
      0.11675201046384255,
      0.1185058454302961,
      0.11902917928806386,
      0.11656207526690412,
      0.11586322995134964,
      0.11561325690176255,
      0.11716454924146844,
      0.11575198739293051,
      0.11819581799457599,
      0.11504396716330588,
      0.11434202309049907,
      0.11484789101740468,
      0.11314302690904406,
      0.11323720804310995,
      0.1164044714764548,
      0.11479149592142951,
      0.11573515963927485,
      0.11436576095374809,
      0.11365403154298181,
      0.1126766141567125,
      0.11267937064981265,
      0.11376498462589292,
      0.11172794530778864,
      0.11237227392997907,
      0.11166018786997352,
      0.11312056361879674,
      0.11041439726390613,
      0.1137169630726892,
      0.11347535052640186,
      0.11334223901158176,
      0.11319988215960497,
      0.10907477057090229,
      0.11223489845055933,
      0.1114868431472974,
      0.11035516147830048,
      0.11081379540398403,
      0.11303359648932428,
      0.11177212619946637,
      0.11006618786570718,
      0.11016415098558761,
      0.10932769111834043,
      0.10962702843502523,
      0.11156827081788069,
      0.1093542028874113,
      0.110010988142565,
      0.11008453891713416,
      0.10951031461212805,
      0.10915245542378227,
      0.10889282756092733,
      0.11042270658794827,
      0.11043385968659705,
      0.10898212231447171,
      0.10952691975053976,
      0.10732828595581392,
      0.10929513021179686,
      0.10722316448706244,
      0.10970536330805612
    ],
    "val_loss": [
      0.3693326948110215,
      0.4925079699404939,
      0.4588625034886206,
      0.45913225003702196,
      0.46038710485675377,
      0.46052501153446246,
      0.5623997327274906,
      0.491505566239357,
      0.5594071482469936,
      0.5272100878868274,
      0.5384087811330122,
      0.5445042415114933,
      0.4975124503681046,
      0.5354339273746856,
      0.4962648676541037,
      0.4821701926028657,
      0.4932315028534678,
      0.4823953197744792,
      0.4817108487297675,
      0.44809525070076217,
      0.4962865993112861,
      0.4973522555506872,
      0.5092848866821049,
      0.47159186792587804,
      0.49317236139388854,
      0.46640943758145065,
      0.4601338990255744,
      0.4841699017914469,
      0.4953298675799798,
      0.4577854421324359,
      0.4433914940871164,
      0.4592587246866283,
      0.49817099457015535,
      0.4164337290975148,
      0.47275009408682406,
      0.44132643698932167,
      0.4278969457584941,
      0.4263549505593534,
      0.4233391266026183,
      0.43777418361452525,
      0.42875742061052496,
      0.42176424000791446,
      0.41629178102858766,
      0.4169048902160393,
      0.4431608452768383,
      0.43228542585215884,
      0.4106138103557918,
      0.419467474725432,
      0.42359074354171755,
      0.41938421590599473,
      0.4293218636227225,
      0.3986286021813661,
      0.4014133312923466,
      0.464314896011067,
      0.42283634984207724,
      0.4212605718545571,
      0.44187428301679876,
      0.42715757382130193,
      0.4149843266981091,
      0.4341721143015844,
      0.46524254961641964,
      0.42971375752351954,
      0.4123781366262607,
      0.4434149658430122,
      0.4114352083848622,
      0.41109917044282673,
      0.4198346414787327,
      0.4117803904289257,
      0.4346662892017536,
      0.4279502048285421,
      0.4182703194147099,
      0.4303521568189838,
      0.423155552505733,
      0.4079873527952297,
      0.43991799638299883,
      0.4032498059872382,
      0.43087937269739046,
      0.40076444320336074,
      0.3918804459943029,
      0.4306983115787278,
      0.405224893674879,
      0.422176833102803,
      0.4239873558461309,
      0.41588344381241027,
      0.4237875601131759,
      0.4232311864099103,
      0.40574115843116165,
      0.4188647553949299,
      0.4193151277874758,
      0.4344094397243625,
      0.4177829383376116,
      0.42589535395542305,
      0.41153820870879165,
      0.4108667740386403,
      0.4322078566886708,
      0.41990182567499357,
      0.4359372843882281,
      0.4147757118690514,
      0.4068559555236451,
      0.4155457702582468,
      0.4389866994169658,
      0.43144044305036167,
      0.41756707710063384,
      0.4117514384185483,
      0.4029558409295396,
      0.4014909353798735,
      0.41517374417382086,
      0.41910336619008803,
      0.4010727198002581,
      0.40724645314102403,
      0.4105413439744961,
      0.440930840926256,
      0.40998594668097127,
      0.41045866862028657,
      0.40625697306530206,
      0.4142651317719214,
      0.41937760335956503,
      0.40402140178366336,
      0.42244350681761783,
      0.41908228154667837,
      0.4215202569069263,
      0.40416075035840454,
      0.4009573327210135,
      0.41512929449181357,
      0.4327990441086763,
      0.42156983947682525,
      0.4486725248262554,
      0.4277785352961032,
      0.4156851033964557
    ],
    "best_epoch": 79,
    "best_val_loss": 0.3918804459943029,
    "best_val_abs_mse": 0.486861914396286,
    "test_loss": 0.5461645929508232,
    "test_abs_mse": 1.256653904914856,
    "tracker": {
      "initial_train_loss": 0.4240759349499316,
      "train_threshold": 0.14135864498331055,
      "best_tracking": true,
      "patience_active": true,
      "patience_best_train": null,
      "patience_best_val": 0.3918804459943029,
      "patience_no_improve_epochs": 50,
      "best_train_loss": Infinity,
      "forced_stop_due_to_threshold": false
    }
  },
  "model_files": {
    "best": "scripts/outputs/mlp-other-rate/trial_33_a82110/best_model.pt",
    "last": "scripts/outputs/mlp-other-rate/trial_33_a82110/last_model.pt"
  },
  "config_path": "/home/amoris/data-analysis-on-dbps/scripts/outputs/mlp-other-rate/trial_33_a82110/config.yaml"
}