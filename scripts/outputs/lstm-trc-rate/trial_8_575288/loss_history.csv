epoch,train_loss,val_loss
1,1.000343,0.354621
2,1.000070,0.346175
3,0.999905,0.357834
4,0.999352,0.349480
5,0.999483,0.348426
6,0.999384,0.354372
7,0.999405,0.353059
8,0.999502,0.350576
9,0.999660,0.345949
10,0.999651,0.354107
11,0.999376,0.353377
12,0.999406,0.355368
13,0.999757,0.352067
14,0.999294,0.353295
15,0.999336,0.353167
16,0.999349,0.350727
17,0.999424,0.354006
18,0.999350,0.353133
19,0.999327,0.348547
20,0.999403,0.351437
21,0.999306,0.351722
22,0.999307,0.350030
23,0.999295,0.351934
24,0.999303,0.350861
25,0.999291,0.353789
26,0.999308,0.351092
27,0.999323,0.349635
28,0.999252,0.352931
29,0.999290,0.350907
30,0.999265,0.351797
31,0.999276,0.351281
32,0.999250,0.352201
33,0.999309,0.351974
34,0.999317,0.351460
35,0.999303,0.352987
36,0.999355,0.350593
37,0.999267,0.352112
38,0.999319,0.352500
39,0.999332,0.350222
40,0.999269,0.351532
41,0.999343,0.353082
42,0.999338,0.350039
43,0.999436,0.350261
44,0.999321,0.351711
45,0.999237,0.351117
46,0.999308,0.349184
47,0.999260,0.351359
48,0.999296,0.352380
49,0.999290,0.350477
50,0.999270,0.351392
