{
  "model_name": "gru-other-value/trial_8_bc40ce",
  "model_type": "GRU",
  "model_format": "torch",
  "model_params": {
    "history_length": 45,
    "units": 265,
    "num_layers": 6,
    "dropout": 0.3224246063653975
  },
  "training_params": {
    "max_epochs": 500,
    "batch_size": 286,
    "learning_rate": 0.0003410464962933181,
    "weight_decay": 0.002336784268403069,
    "checkpoint_interval": 10,
    "seed": 947
  },
  "data_csv": "data/time_aligned_data.csv",
  "timestamp_column": "Date, Time",
  "columns": [
    "TRC-DT",
    "TRC-RT",
    "TRC-PPL1",
    "TRC-PPL2",
    "pH-DT",
    "pH-RT",
    "pH-PPL1",
    "pH-PPL2",
    "cond-DT",
    "cond-PPL1",
    "cond-PPL2",
    "fDOM-RT",
    "fDOM-PPL1",
    "fDOM-PPL2",
    "DO-RT",
    "DO-PPL1",
    "DO-PPL2",
    "TOC-RT",
    "TOC-PPL1",
    "TOC-PPL2",
    "DOC-RT",
    "DOC-PPL1",
    "DOC-PPL2",
    "minutes_since_start"
  ],
  "feature_columns": [
    "TRC-DT",
    "TRC-RT",
    "pH-DT",
    "pH-RT",
    "cond-DT",
    "fDOM-RT",
    "DO-RT",
    "TOC-RT",
    "DOC-RT",
    "minutes_since_start"
  ],
  "target_columns": [
    "TOC-PPL1",
    "TOC-PPL2",
    "DOC-PPL1",
    "DOC-PPL2",
    "pH-PPL1",
    "pH-PPL2"
  ],
  "split_boundaries": {
    "train_end": 7796,
    "val_end": 9466,
    "test_end": 11138
  },
  "dataset_sizes": {
    "train": 7752,
    "val": 1670,
    "test": 1672
  },
  "input_dim": 10,
  "sequence_length": 45,
  "training_history": {
    "epochs": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46,
      47,
      48,
      49,
      50,
      51,
      52,
      53,
      54,
      55,
      56,
      57,
      58,
      59,
      60,
      61,
      62,
      63,
      64,
      65,
      66,
      67,
      68,
      69,
      70,
      71,
      72,
      73,
      74,
      75,
      76,
      77,
      78,
      79,
      80,
      81,
      82,
      83,
      84,
      85,
      86,
      87,
      88,
      89,
      90,
      91,
      92,
      93,
      94,
      95,
      96,
      97,
      98,
      99,
      100,
      101,
      102,
      103,
      104,
      105,
      106,
      107,
      108,
      109,
      110,
      111,
      112,
      113,
      114,
      115,
      116,
      117,
      118,
      119,
      120,
      121,
      122,
      123,
      124,
      125,
      126,
      127,
      128,
      129,
      130,
      131,
      132,
      133,
      134
    ],
    "train_loss": [
      0.5036359573757685,
      0.30799982465334597,
      0.2690521544278037,
      0.25179470289340944,
      0.24028629706970678,
      0.23993228614653228,
      0.23196219303044496,
      0.2297319071473106,
      0.23013386283727252,
      0.22475416535824938,
      0.221684624736031,
      0.22392927630351792,
      0.21641754516245656,
      0.21834633109554788,
      0.20752594536179975,
      0.20748200310226933,
      0.19948305041597478,
      0.19802035409399724,
      0.1961320386959242,
      0.19495475341030566,
      0.1926140362633271,
      0.1902204863957761,
      0.18522313232576884,
      0.18393924919229052,
      0.18207340380724737,
      0.1855731465314564,
      0.17963849972276127,
      0.1746105395691898,
      0.17346155024503654,
      0.1707865793653932,
      0.16935754600062705,
      0.16859741197558747,
      0.16578029954212,
      0.16393709611013085,
      0.16292258560795903,
      0.159890426021027,
      0.15643818590355607,
      0.15411889535001064,
      0.1532629946990648,
      0.15388646894264332,
      0.1499463406663438,
      0.15282929573358028,
      0.14765702915388487,
      0.15368486973154286,
      0.1528368428891565,
      0.14823362375229934,
      0.14875800794200017,
      0.1578414535920748,
      0.14711405755903945,
      0.14742927608859674,
      0.14587442750462992,
      0.14453129947831386,
      0.14328495760989385,
      0.14500897693360307,
      0.15145977230702207,
      0.14468991304906345,
      0.1448287565503327,
      0.14101543349778073,
      0.14249906191454337,
      0.1422841040008116,
      0.13978177218114568,
      0.13784077065017447,
      0.13932876473617567,
      0.13954351655946673,
      0.13941234747722545,
      0.1367018166122145,
      0.13649713537502953,
      0.13844684920090147,
      0.13564780621498176,
      0.1330441763534204,
      0.13497602233184566,
      0.1302211588350489,
      0.13054167851036794,
      0.13114702641787113,
      0.13645628215773925,
      0.12978793718343673,
      0.13043685445813819,
      0.12708819135485297,
      0.12908306198292657,
      0.1272595751669231,
      0.12979099152165668,
      0.12376511461986661,
      0.12387798535104137,
      0.12054222881586374,
      0.11866658045242322,
      0.11777558004900705,
      0.11737836071196847,
      0.12203884407900989,
      0.12244869909903768,
      0.11278690102099757,
      0.11426665329627146,
      0.11327703069353436,
      0.11227535865470475,
      0.11817085522662375,
      0.11564516662874703,
      0.10933340629764571,
      0.10632090982130438,
      0.106224167309663,
      0.10768347070336157,
      0.10442531681297611,
      0.10293928718327,
      0.10551620979473318,
      0.10270288313467006,
      0.10432453183851313,
      0.10340316054075803,
      0.09901215031866073,
      0.10092022808189854,
      0.09819642950706683,
      0.10129377552883852,
      0.10398930372330058,
      0.09965355903048634,
      0.09914862182879411,
      0.10135637691015736,
      0.09751197250350402,
      0.09775560312126874,
      0.0994267199223071,
      0.10423561465091258,
      0.0956172351220813,
      0.10425365759277,
      0.1003124114929461,
      0.10039741068048127,
      0.09255678642837194,
      0.0929717468549531,
      0.09148163962782475,
      0.09561272261143346,
      0.09478650285724946,
      0.09071487448609773,
      0.09532171069722242,
      0.09603663923370703,
      0.09377035857884691,
      0.09110136479869776,
      0.09018005483944289,
      0.09156886487238117,
      0.09520460756724342
    ],
    "val_loss": [
      0.4728335132142027,
      0.3916765075957704,
      0.4216201686752057,
      0.3928743813209191,
      0.5253264140583084,
      0.3816418254625298,
      0.3652363273911847,
      0.3941331057491417,
      0.3957971901593808,
      0.41589555749279294,
      0.43364161625950637,
      0.40837375662640896,
      0.5302072365216153,
      0.5587975207738534,
      0.5168212243718302,
      0.5295421394223938,
      0.6419199354098942,
      0.5456809955978108,
      0.588199183344841,
      0.5949032820270447,
      0.6063024138262172,
      0.5699410632163465,
      0.5347092321176015,
      0.6567509429183549,
      0.6449354542943532,
      0.6158015125704382,
      0.6261049583108126,
      0.5915530298462884,
      0.612725188650057,
      0.5980615581580979,
      0.641598466333158,
      0.6693456657453926,
      0.6466396783640285,
      0.6327797999995911,
      0.6628999965276546,
      0.6595583168510906,
      0.6273231454505892,
      0.6450617411982513,
      0.6663511416869249,
      0.6793624944494157,
      0.6573954883985177,
      0.6605155312372539,
      0.6798584072175854,
      0.600859053638167,
      0.6700258798049595,
      0.7044712322022386,
      0.7420470579030984,
      0.6211446585590968,
      0.6385981791569088,
      0.6193195560378228,
      0.6414883319756942,
      0.6422660929000307,
      0.6860219088321674,
      0.665972168886376,
      0.6448324481616478,
      0.6904098415178453,
      0.6431155809920704,
      0.6263094411014083,
      0.6229843564911517,
      0.6426105338210117,
      0.6487310348513597,
      0.6303154055854517,
      0.6486034497440218,
      0.7079696252228257,
      0.6121530077503827,
      0.6114054817817882,
      0.5735728594714296,
      0.5744295883678391,
      0.6700095707427955,
      0.6458194329620836,
      0.5875015288769841,
      0.5575946335753281,
      0.5549838032461926,
      0.5387790914245708,
      0.6224013427149749,
      0.6251677637417873,
      0.5543386359236198,
      0.5628048718243301,
      0.5418089116851013,
      0.5329064999957999,
      0.6187938549115273,
      0.5187281373821333,
      0.5457457222356767,
      0.4864202901988686,
      0.5055683947698085,
      0.5660209862414948,
      0.52698108123805,
      0.5818689668785312,
      0.560170505289546,
      0.5449985496833653,
      0.5455923377039904,
      0.5323455059332048,
      0.5547625362337706,
      0.586865628640095,
      0.5394109965352241,
      0.5170914805934814,
      0.5195800099365725,
      0.5486233583824364,
      0.588829051948593,
      0.5548410960121783,
      0.5693226976398222,
      0.5919854200082625,
      0.5576491784192845,
      0.5573184135698986,
      0.5445742303382851,
      0.5483098498450781,
      0.6109194991385152,
      0.5516246838751667,
      0.61332167115754,
      0.5347925852604969,
      0.5933402971236291,
      0.554900567501248,
      0.5651279102631672,
      0.5682591465954295,
      0.5905949691366292,
      0.5647461577803788,
      0.5914601995916424,
      0.5295603723583108,
      0.5541649839210653,
      0.5798179015874149,
      0.6271097317070304,
      0.6093547872262086,
      0.6094666927160617,
      0.5712028380907225,
      0.599661986833204,
      0.6096111160195516,
      0.610019363097088,
      0.5896222471923172,
      0.6295518606365798,
      0.6407565732559044,
      0.5958264327245558,
      0.6405909446899049,
      0.6171705612343943,
      0.6226426526993335
    ],
    "best_epoch": 84,
    "best_val_loss": 0.4864202901988686,
    "test_loss": 0.4337171796513231,
    "tracker": {
      "initial_train_loss": 0.5036359573757685,
      "train_threshold": 0.1678786524585895,
      "best_tracking": true,
      "patience_active": true,
      "patience_best_train": null,
      "patience_best_val": 0.4864202901988686,
      "patience_no_improve_epochs": 50,
      "best_train_loss": Infinity,
      "forced_stop_due_to_threshold": false
    }
  },
  "model_files": {
    "best": "scripts/outputs/gru-other-value/trial_8_bc40ce/best_model.pt",
    "last": "scripts/outputs/gru-other-value/trial_8_bc40ce/last_model.pt"
  },
  "config_path": "/home/amoris/data-analysis-on-dbps/scripts/outputs/gru-other-value/trial_8_bc40ce/config.yaml"
}