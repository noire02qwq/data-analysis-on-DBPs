{
  "model_name": "gru-other-value/trial_1_ec68de",
  "model_type": "GRU",
  "model_format": "torch",
  "model_params": {
    "history_length": 140,
    "units": 373,
    "num_layers": 8,
    "dropout": 0.4019474556737168
  },
  "training_params": {
    "max_epochs": 500,
    "batch_size": 175,
    "learning_rate": 0.0004327802624993831,
    "weight_decay": 0.0024832938296260464,
    "checkpoint_interval": 10,
    "seed": 947
  },
  "data_csv": "data/time_aligned_data.csv",
  "timestamp_column": "Date, Time",
  "columns": [
    "TRC-DT",
    "TRC-RT",
    "TRC-PPL1",
    "TRC-PPL2",
    "pH-DT",
    "pH-RT",
    "pH-PPL1",
    "pH-PPL2",
    "cond-DT",
    "cond-PPL1",
    "cond-PPL2",
    "fDOM-RT",
    "fDOM-PPL1",
    "fDOM-PPL2",
    "DO-RT",
    "DO-PPL1",
    "DO-PPL2",
    "TOC-RT",
    "TOC-PPL1",
    "TOC-PPL2",
    "DOC-RT",
    "DOC-PPL1",
    "DOC-PPL2",
    "minutes_since_start"
  ],
  "feature_columns": [
    "TRC-DT",
    "TRC-RT",
    "pH-DT",
    "pH-RT",
    "cond-DT",
    "fDOM-RT",
    "DO-RT",
    "TOC-RT",
    "DOC-RT",
    "minutes_since_start"
  ],
  "target_columns": [
    "TOC-PPL1",
    "TOC-PPL2",
    "DOC-PPL1",
    "DOC-PPL2",
    "pH-PPL1",
    "pH-PPL2"
  ],
  "split_boundaries": {
    "train_end": 7796,
    "val_end": 9466,
    "test_end": 11138
  },
  "dataset_sizes": {
    "train": 7657,
    "val": 1670,
    "test": 1672
  },
  "input_dim": 10,
  "sequence_length": 140,
  "training_history": {
    "epochs": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46,
      47,
      48,
      49,
      50,
      51,
      52,
      53,
      54,
      55,
      56,
      57,
      58,
      59,
      60,
      61,
      62,
      63,
      64,
      65,
      66,
      67,
      68,
      69,
      70,
      71,
      72,
      73,
      74,
      75,
      76,
      77,
      78,
      79,
      80,
      81,
      82,
      83,
      84,
      85,
      86,
      87,
      88,
      89,
      90,
      91,
      92,
      93,
      94,
      95,
      96,
      97,
      98,
      99,
      100,
      101,
      102,
      103,
      104,
      105,
      106,
      107,
      108,
      109,
      110,
      111,
      112,
      113,
      114,
      115,
      116,
      117,
      118,
      119,
      120,
      121,
      122,
      123,
      124,
      125,
      126,
      127,
      128,
      129,
      130,
      131,
      132,
      133,
      134,
      135,
      136,
      137,
      138,
      139,
      140,
      141,
      142,
      143,
      144,
      145,
      146,
      147,
      148,
      149,
      150,
      151,
      152,
      153,
      154,
      155,
      156
    ],
    "train_loss": [
      0.44253176607972916,
      0.2815865278185636,
      0.25575919027418476,
      0.2434462327254695,
      0.2388313776253097,
      0.24301369099217202,
      0.24086706788301374,
      0.22961785447446487,
      0.22560193177822974,
      0.21908360054120238,
      0.213393094916949,
      0.20626783112531794,
      0.2051570987859438,
      0.2069163584586175,
      0.19975368029745563,
      0.20395722083257947,
      0.1981733050676586,
      0.20215231887592383,
      0.19796496985848241,
      0.19795141837623983,
      0.1949478629477827,
      0.1903808891006467,
      0.19792098984505654,
      0.1958849554322198,
      0.19295991263992718,
      0.19146298380383503,
      0.19220977971996575,
      0.19489069293128208,
      0.19027869159907232,
      0.1869308085779118,
      0.1862192478206554,
      0.19498502686220812,
      0.1832807285559618,
      0.18820597883520665,
      0.18163460370095294,
      0.1764949874360583,
      0.1831260037177752,
      0.17450877417117822,
      0.16570290234788865,
      0.17009478092847488,
      0.1856078168104099,
      0.17935903563237274,
      0.16984940445289234,
      0.16933603323796592,
      0.18484029284777773,
      0.16447622148400112,
      0.1656455170984475,
      0.16282318256317643,
      0.1546686780434046,
      0.16263730324434053,
      0.16014441557445552,
      0.18511701210510081,
      0.26434191615965746,
      0.19867417589754383,
      0.16607085245348727,
      0.1569007463977996,
      0.14750054692909115,
      0.14301874651965857,
      0.14265343610990822,
      0.15126624005347145,
      0.13993342156951696,
      0.13871234222668588,
      0.1409714273622862,
      0.15473179995869632,
      0.13909408284806857,
      0.1311521452093959,
      0.1331753733128687,
      0.1356741602331754,
      0.14125288417982468,
      0.13661872269287734,
      0.13409307185719557,
      0.13625827104347532,
      0.13222018846300823,
      0.13037857096956063,
      0.128403261567596,
      0.1361792484740839,
      0.13440468456254112,
      0.13099671354882475,
      0.13254741146387936,
      0.13338000235007047,
      0.1229918756226312,
      0.13138283782151966,
      0.1294964838729712,
      0.12615253527894318,
      0.12316007135379746,
      0.12458895553091698,
      0.12448062457069337,
      0.1314965942493848,
      0.12484966612553432,
      0.12171713821419665,
      0.12208296689711462,
      0.12555323512362362,
      0.1298363434458053,
      0.12781631905112142,
      0.12824511123910903,
      0.12124009033796665,
      0.12521106214113548,
      0.1237331965792984,
      0.1269005185132536,
      0.12428486856683141,
      0.12591129073753235,
      0.12037291087524471,
      0.11887345791536381,
      0.11755947157423381,
      0.11808505690726782,
      0.12180956018037985,
      0.12805062627004585,
      0.12430014194665479,
      0.12160885446498518,
      0.12661199230130044,
      0.1189646559692136,
      0.11489373273768998,
      0.11992907102976422,
      0.1226788623638201,
      0.11929568276733678,
      0.11700281966319513,
      0.113820961032506,
      0.11849770298748488,
      0.11610802691257138,
      0.11630390033143392,
      0.11360010215145373,
      0.12030962797700853,
      0.1223977166452404,
      0.11521464310303298,
      0.12021941657483275,
      0.11348217495142893,
      0.12055520615830533,
      0.11530176406626261,
      0.12029320956545336,
      0.12430070382863403,
      0.11392689870316007,
      0.11378092548119208,
      0.10925892965455303,
      0.11192657225212478,
      0.10768004970772702,
      0.11437150663373301,
      0.11081778473224779,
      0.11713466326530599,
      0.111185425915582,
      0.10991991888520082,
      0.11121054263623713,
      0.10961492291330684,
      0.11264052776824374,
      0.11523746002299583,
      0.11756474276902673,
      0.11702845229252645,
      0.11367741671051605,
      0.11217055404946778,
      0.11703766819463786,
      0.11067501787352658,
      0.1081578357295282,
      0.11450284545389997,
      0.1100528741505231,
      0.11037647492956894,
      0.10667821531415966,
      0.11818963615424455
    ],
    "val_loss": [
      0.4967866084443595,
      0.40639977159346646,
      0.4229520626455367,
      0.4203768717180826,
      0.4697785490822649,
      0.5464139975116639,
      0.43749059741190094,
      0.4794261516657418,
      0.49738449616703445,
      0.5058492865569577,
      0.7422380230116273,
      0.7475901507956539,
      0.7017945271587658,
      0.6516787693975215,
      0.629866896141432,
      0.7022033568895506,
      0.5838124712114919,
      0.6642415348641173,
      0.605799456199486,
      0.7023420800973555,
      0.6604531186069557,
      0.7210396592042403,
      0.712743957777937,
      0.6974326489035955,
      0.6356628376120579,
      0.63096901172114,
      0.7186301497820609,
      0.6822332263795915,
      0.6650409271974049,
      0.8571517366001349,
      0.6786814622893305,
      0.7346560168498291,
      0.659651117678174,
      0.6664985115835053,
      0.6329868607847633,
      0.7075817207554858,
      0.7058096355619188,
      0.6948454138672281,
      0.7993215070200894,
      0.7009750830377647,
      0.6126843292913037,
      0.49975528112964,
      0.669638428502454,
      0.5693034268692582,
      0.49704136922509373,
      0.5746706233186993,
      0.5765702319359352,
      0.6234463863967065,
      0.6005943415764563,
      0.521336361676633,
      0.5148272937955614,
      0.8236576107269276,
      0.5809199659231894,
      0.6258778413255771,
      0.6373428902895508,
      0.5916676238669963,
      0.5312449097588747,
      0.5943645636166284,
      0.505877161141998,
      0.5230670432576876,
      0.5733003224619848,
      0.5912926258619674,
      0.634055094179993,
      0.5008281254304383,
      0.5765099233897504,
      0.5573407559694644,
      0.49961270206791913,
      0.5141417625287693,
      0.541545095022567,
      0.5136544926616247,
      0.5252741100545415,
      0.4822461205551367,
      0.4877961320122202,
      0.5153189340305185,
      0.5183671837126067,
      0.5110049608894094,
      0.48026408705704227,
      0.5424580561989796,
      0.5378310742849362,
      0.5840663171546188,
      0.5338333556484319,
      0.5120704956129639,
      0.5344795677476301,
      0.5813951825131913,
      0.5307671070812705,
      0.4801904496006266,
      0.5133038247729133,
      0.4950815841421753,
      0.5719738616022522,
      0.5729089593905174,
      0.4784241241073894,
      0.4921025149524212,
      0.49983775791918444,
      0.5359424848310248,
      0.5146809465023215,
      0.4956553274553693,
      0.557510898342568,
      0.5028928080183304,
      0.5306069293689585,
      0.5314301562880328,
      0.5512700847790627,
      0.5361647617718773,
      0.5399208147814888,
      0.5217089954830573,
      0.4916910657625712,
      0.44779074165278565,
      0.5316888624411857,
      0.506291002905119,
      0.5271064575293107,
      0.4731564374562509,
      0.5424147157031975,
      0.48420123693471895,
      0.5269401146177046,
      0.543777091417484,
      0.5407555731916855,
      0.5178395579318087,
      0.5407092002872935,
      0.5226815620180733,
      0.530812236117924,
      0.5176575109362602,
      0.5010139526364332,
      0.5370463338976135,
      0.4984315374118839,
      0.4989157408074348,
      0.5326926078268154,
      0.5310194451964187,
      0.4997546426773428,
      0.5303901576710318,
      0.5107859848011397,
      0.5379086370596629,
      0.49136755793929815,
      0.5126499829222699,
      0.5393417554121175,
      0.5211568420786343,
      0.533949824172758,
      0.5326294452621194,
      0.5015741206794798,
      0.5424488209589513,
      0.5123819499137159,
      0.5667267753826287,
      0.5173022836268305,
      0.5223838251061782,
      0.5252398224380201,
      0.4948687620104073,
      0.5379103392496437,
      0.5025139858578137,
      0.5119939402058096,
      0.485232255750609,
      0.49273860809242653,
      0.5215921379893482,
      0.49651518129481526,
      0.47961844622107325,
      0.5009395979389459,
      0.5126309747512112,
      0.5263596225953745,
      0.5195473713610701
    ],
    "best_epoch": 106,
    "best_val_loss": 0.44779074165278565,
    "test_loss": 0.48838015769621684,
    "tracker": {
      "initial_train_loss": 0.44253176607972916,
      "train_threshold": 0.14751058869324304,
      "best_tracking": true,
      "patience_active": true,
      "patience_best_train": null,
      "patience_best_val": 0.44779074165278565,
      "patience_no_improve_epochs": 50,
      "best_train_loss": Infinity,
      "forced_stop_due_to_threshold": false
    }
  },
  "model_files": {
    "best": "scripts/outputs/gru-other-value/trial_1_ec68de/best_model.pt",
    "last": "scripts/outputs/gru-other-value/trial_1_ec68de/last_model.pt"
  },
  "config_path": "/home/amoris/data-analysis-on-dbps/scripts/outputs/gru-other-value/trial_1_ec68de/config.yaml"
}