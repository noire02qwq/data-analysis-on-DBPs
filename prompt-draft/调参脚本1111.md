# 调参脚本

## 原始网格调参

移植自1109

在双输出回归实验跑通后进行，旨在利用空闲保持运行。对于每个模型编写一个调参网格yaml，来描述config的yaml中哪几个超参数是可调的，调整的最小值、最大值、间隔步长是多少。在网格调参时，每确定一组超参数，在outputs中对应模型的文件夹生成一个名为编号的文件夹，里面先生成这组超参数的yaml，然后按照这个yaml执行训练，训练一切结果保存在这个文件夹中。同时在outputs中的对应模型的文件夹里编写一个调参结果csv，每行包含编号、几个超参的值、best的val loss。接下来是每个模型的超参可调整值描述

- LSTM与RNN
    - history_length: 32-128 间隔16 初始64
    - units: 128-1024 间隔64 初始192
    - num_layers: 4, 8, 12, 16, 24, 32 初始8
    - dropout: 0.2-0.5 间隔0.05 初始0.3
    - batch_size: 128-256 间隔32 初始192
    - learning_rate: 0.0004-0.0012 间隔0.0002 初始0.0006
    - weight_decay: 0.00-0.01 间隔0.002 初始0.00

## 爬山调参

基于回归实验1109中调参脚本的写法，但是使用爬山算法来优化网格枚举。具体算法如下：

1. 每个超参的取值序列基础上确定一个起始值，要保证起始值在这个范围内。起始值也写在网格调参的yaml中。
2. 参数按照起始值训练，记录best val-loss作为初始状态，编号和网格枚举类似，从1开始。
3. 随机打乱参数顺序，打乱后从第一个超参开始依次尝试:
4. 把该超参数在取值序列上向左/右移动一格(若当前状态在左/右边界上则只尝试单侧移动)进行训练，保存在两个临时文件夹中，记录val-loss
    1. 若两个方向移动后best val-loss比当前状态都不下降，则不改动，删除产生的两个临时文件夹，尝试调整下一个超参数，回到第4步
    2. 若至少有一个方向移动后best val-loss下降，则保存best val-loss最低方向的改动，记录新的超参数取值和这一best val-loss作为新的状态，把这一有效的移动在终端输出，编号+1把这一临时文件夹重命名，删除没采用的移动方向的临时文件夹，回到第3步
5. 停止规则：参考4.1，若所有超参数均不能通过左右移动使best val-loss下降，视作当前状态为最优(山顶)，停止调参。

## 训练脚本的修改(在原程序上覆盖)

不再每10轮epoch保存checkpoint，只最后保存best的模型

train loss和val loss全部使用mse