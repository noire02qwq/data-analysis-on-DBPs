epoch,train_loss,val_loss
1,1.000908,0.340584
2,1.001106,0.345376
3,1.000910,0.345663
4,1.000926,0.351160
5,1.000830,0.348373
6,1.000786,0.347440
7,1.000753,0.347482
8,1.000844,0.345896
9,1.000698,0.347055
10,1.000695,0.348008
11,1.000707,0.346972
12,1.000729,0.348815
13,1.000690,0.348429
14,1.000685,0.347832
15,1.000666,0.348132
16,1.000706,0.348377
17,1.000681,0.347486
18,1.000695,0.348253
19,1.000695,0.347793
20,1.000680,0.347392
21,1.000692,0.347018
22,1.000736,0.348706
23,1.000690,0.347501
24,1.000696,0.347355
25,1.000663,0.348205
26,1.000685,0.348990
27,1.000675,0.348030
28,1.000666,0.348085
29,1.000686,0.347872
30,1.000654,0.348741
31,1.000658,0.348948
32,1.000672,0.349097
33,1.000667,0.348448
34,1.000661,0.348855
35,1.000670,0.349078
36,1.000669,0.348887
37,1.000662,0.348707
38,1.000660,0.348907
39,1.000659,0.348858
40,1.000697,0.347967
41,1.000670,0.348988
42,1.000655,0.348974
43,1.000654,0.348892
44,1.000668,0.349158
45,1.000664,0.348538
46,1.000670,0.349344
47,1.000657,0.349002
48,1.000655,0.349138
49,1.000656,0.349094
50,1.000656,0.349040
