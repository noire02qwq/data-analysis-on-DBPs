epoch,train_loss,val_loss
1,1.000194,0.343137
2,0.999606,0.360325
3,0.999816,0.356372
4,0.999454,0.347864
5,0.999635,0.350136
6,0.999499,0.353655
7,0.999330,0.357978
8,0.999387,0.346073
9,0.999499,0.350029
10,0.999351,0.352185
11,0.999258,0.350025
12,0.999297,0.351953
13,0.999367,0.348495
14,0.999258,0.351109
15,0.999422,0.349046
16,0.999283,0.352151
17,0.999211,0.349520
18,0.999250,0.349355
19,0.999268,0.350784
20,0.999365,0.353063
21,0.999297,0.354485
22,0.999418,0.355772
23,0.999211,0.351192
24,0.999285,0.350698
25,0.999308,0.350324
26,0.999355,0.350777
27,0.999381,0.351284
28,0.999246,0.351575
29,0.999220,0.349970
30,0.999216,0.350412
31,0.999212,0.352609
32,0.999249,0.354172
33,0.999300,0.350928
34,0.999230,0.353494
35,0.999269,0.353137
36,0.999200,0.350370
37,0.999223,0.350486
38,0.999209,0.350541
39,0.999261,0.350442
40,0.999297,0.351158
41,0.999222,0.351069
42,0.999261,0.347412
43,0.999230,0.349266
44,0.999273,0.350884
45,0.999262,0.351466
46,0.999200,0.350051
47,0.999276,0.351422
48,0.999297,0.352695
49,0.999351,0.353711
50,0.999242,0.352039
