epoch,train_loss,val_loss
1,0.685240,0.259649
2,0.233753,0.248960
3,0.157863,0.246357
4,0.147012,0.255025
5,0.138828,0.246385
6,0.133508,0.230640
7,0.140785,0.263024
8,0.147651,0.263331
9,0.128234,0.246370
10,0.122537,0.244033
11,0.129727,0.532716
12,0.123866,0.280188
13,0.121989,0.283129
14,0.116626,0.281477
15,0.118492,0.285495
16,0.120207,0.277927
17,0.118018,0.278531
18,0.114908,0.332293
19,0.115987,0.284891
20,0.118422,0.356280
21,0.114841,0.280198
22,0.122632,0.277819
23,0.119137,0.278894
24,0.110852,0.272436
25,0.115525,0.282842
26,0.114131,0.278910
27,0.110156,0.281047
28,0.113501,0.269137
29,0.114483,0.291111
30,0.113794,0.281946
31,0.110832,0.293467
32,0.115506,0.291200
33,0.119497,0.284378
34,0.112485,0.278949
35,0.113424,0.276401
36,0.110410,0.277984
37,0.130928,0.291908
38,0.111869,0.300501
39,0.110894,0.278860
40,0.111733,0.273642
41,0.110123,0.292291
42,0.108499,0.289556
43,0.106584,0.277789
44,0.113161,0.279054
45,0.106846,0.330703
46,0.106772,0.346565
47,0.103863,0.280249
48,0.110309,0.282568
49,0.114329,0.323288
50,0.106360,0.334079
51,0.104599,0.407760
52,0.114198,0.302672
53,0.107329,0.275305
54,0.105649,0.283057
55,0.102745,0.277663
56,0.126660,0.259261
57,0.112721,0.275958
58,0.102408,0.379927
59,0.111497,0.280314
60,0.103065,0.347967
61,0.107729,0.420247
62,0.109566,0.304437
63,0.104176,0.310433
64,0.102210,0.368946
65,0.103720,0.299210
66,0.103885,0.331249
67,0.108841,0.274302
68,0.105332,0.282970
69,0.113317,0.272516
70,0.106877,0.330457
71,0.107293,0.269560
72,0.102059,0.274813
73,0.102560,0.315398
74,0.103288,0.312692
75,0.102466,0.422270
76,0.102465,0.378033
77,0.102492,0.367626
78,0.099387,0.417285
79,0.100207,0.347397
80,0.098774,0.439126
