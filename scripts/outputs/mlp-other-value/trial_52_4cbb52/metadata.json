{
  "model_name": "mlp-other-value/trial_52_4cbb52",
  "model_type": "MLP",
  "model_format": "torch",
  "model_params": {
    "history_length": 1,
    "hidden_layers": [
      512,
      256,
      128
    ],
    "dropout": 0.30045244703802654,
    "mid_layer_count": 7,
    "mid_layer_size": 474
  },
  "training_params": {
    "max_epochs": 400,
    "batch_size": 166,
    "learning_rate": 0.0018799403851741919,
    "weight_decay": 0.0004312680398126577,
    "checkpoint_interval": 10,
    "seed": 42
  },
  "data_csv": "data/time_aligned_data.csv",
  "timestamp_column": "Date, Time",
  "columns": [
    "TRC-DT",
    "TRC-RT",
    "TRC-PPL1",
    "TRC-PPL2",
    "pH-DT",
    "pH-RT",
    "pH-PPL1",
    "pH-PPL2",
    "cond-DT",
    "cond-PPL1",
    "cond-PPL2",
    "fDOM-RT",
    "fDOM-PPL1",
    "fDOM-PPL2",
    "DO-RT",
    "DO-PPL1",
    "DO-PPL2",
    "TOC-RT",
    "TOC-PPL1",
    "TOC-PPL2",
    "DOC-RT",
    "DOC-PPL1",
    "DOC-PPL2",
    "minutes_since_start"
  ],
  "feature_columns": [
    "TRC-DT",
    "TRC-RT",
    "pH-DT",
    "pH-RT",
    "cond-DT",
    "fDOM-RT",
    "DO-RT",
    "TOC-RT",
    "DOC-RT",
    "minutes_since_start"
  ],
  "target_columns": [
    "TOC-PPL1",
    "TOC-PPL2",
    "DOC-PPL1",
    "DOC-PPL2",
    "pH-PPL1",
    "pH-PPL2"
  ],
  "split_boundaries": {
    "train_end": 7796,
    "val_end": 9466,
    "test_end": 11138
  },
  "dataset_sizes": {
    "train": 7796,
    "val": 1670,
    "test": 1672
  },
  "input_dim": 10,
  "sequence_length": null,
  "training_history": {
    "epochs": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46,
      47,
      48,
      49,
      50,
      51,
      52,
      53,
      54,
      55,
      56,
      57,
      58,
      59,
      60,
      61,
      62,
      63,
      64,
      65,
      66,
      67,
      68,
      69,
      70,
      71,
      72,
      73,
      74,
      75,
      76,
      77,
      78,
      79,
      80,
      81,
      82,
      83,
      84,
      85,
      86,
      87,
      88,
      89,
      90,
      91,
      92,
      93,
      94,
      95,
      96,
      97,
      98,
      99,
      100,
      101,
      102,
      103,
      104,
      105,
      106,
      107,
      108,
      109,
      110,
      111,
      112,
      113,
      114,
      115,
      116,
      117,
      118,
      119,
      120,
      121,
      122,
      123,
      124,
      125,
      126,
      127,
      128,
      129,
      130,
      131,
      132,
      133,
      134,
      135,
      136,
      137,
      138,
      139,
      140,
      141,
      142,
      143,
      144,
      145,
      146,
      147,
      148,
      149,
      150
    ],
    "train_loss": [
      0.35503354064619436,
      0.23866777783742374,
      0.21746089291486942,
      0.20153959582499079,
      0.18914437506552412,
      0.18427041872548347,
      0.17581730792389705,
      0.17103395992560286,
      0.16455296088313984,
      0.1602751241357526,
      0.15606430351061354,
      0.1550343266247235,
      0.14983190933654345,
      0.14593414667938354,
      0.14649159398566397,
      0.14279214995581105,
      0.14091388545184946,
      0.13786490330746873,
      0.13643502313481226,
      0.137599471621999,
      0.1325506650221048,
      0.13217948407141167,
      0.12827099737576975,
      0.13038733294410482,
      0.12629742492248364,
      0.12588555842691534,
      0.12476131898576079,
      0.12624985441611386,
      0.1250026743627775,
      0.12334575761528184,
      0.12217744897617139,
      0.12093370147489045,
      0.11937837127984151,
      0.11912479252578846,
      0.11745026185267579,
      0.11739037446811482,
      0.11697681044816603,
      0.1171921692184603,
      0.11950815515090955,
      0.11554347521115597,
      0.11457672332909977,
      0.1151650595383622,
      0.11526093735579897,
      0.1120934686224512,
      0.11373636517778857,
      0.11255982863010046,
      0.11089057873278903,
      0.11308128838969721,
      0.10970513203232456,
      0.11120762771187592,
      0.11100153972310185,
      0.10960381710686826,
      0.11132373954773928,
      0.1124151362351817,
      0.10811872985796968,
      0.10940555657421154,
      0.10693078210578325,
      0.11066386106879421,
      0.10827430709049664,
      0.10745030620530546,
      0.10825694581563501,
      0.1078763030480886,
      0.10672483049054034,
      0.10860155359591014,
      0.10666138634636134,
      0.10517129665188082,
      0.10508305134644504,
      0.10820787574134465,
      0.10621265216270369,
      0.10640643890077423,
      0.10530692790795008,
      0.10703073907978049,
      0.10587099952582153,
      0.10306032808385732,
      0.10335841232871569,
      0.10455822281199213,
      0.10513661094602406,
      0.10417717525887453,
      0.10472110586442539,
      0.10406911772044578,
      0.10439383380952219,
      0.10411448812656002,
      0.1041311925805612,
      0.10318985883588054,
      0.1031337010766403,
      0.10278466318267135,
      0.1022953539991177,
      0.10128648877870186,
      0.10209949497704998,
      0.10274879993911949,
      0.10348476098134556,
      0.10239584798068802,
      0.10212868125989781,
      0.10332897641319015,
      0.1047962089382177,
      0.10252987261213603,
      0.10251011179350106,
      0.10375636901081434,
      0.10212841388765147,
      0.10457941599805701,
      0.10429194678345541,
      0.10237116779675234,
      0.10276010112426964,
      0.09920224825492757,
      0.10174082086568248,
      0.10296007593268917,
      0.10200095819113437,
      0.09907195432108754,
      0.10124471648694186,
      0.1009292336588887,
      0.10054454837221864,
      0.10026744759429902,
      0.10258641624569954,
      0.10108035267844391,
      0.10110089064347799,
      0.10266787838429717,
      0.09885166902161978,
      0.09791640277900349,
      0.09765207819858535,
      0.09985325683487446,
      0.09975419100612906,
      0.09994937893884374,
      0.09988199828671394,
      0.10243805313818329,
      0.10079667618973186,
      0.09885459367650298,
      0.0984393643313735,
      0.09928961654980713,
      0.10268031438767695,
      0.10112859340639772,
      0.09839229913989417,
      0.0995049008697256,
      0.10059306728703783,
      0.09888455307081517,
      0.0991800751722489,
      0.09885143359435895,
      0.10054069849590327,
      0.10080135984106392,
      0.09760796259027864,
      0.09736369558255938,
      0.10013043402807416,
      0.09976839519972187,
      0.09927736334177761,
      0.0982581384388253,
      0.09951645219980478,
      0.10011937009938073,
      0.09933322000046337,
      0.09795339911480389,
      0.09925141432664222,
      0.09851418225321175
    ],
    "val_loss": [
      0.3999329706152042,
      0.5656268047090776,
      0.5299271089588097,
      0.5074901982517299,
      0.5051980714776558,
      0.5581829313746469,
      0.6050232962933842,
      0.5647875459965118,
      0.5578133658556167,
      0.5821333026457689,
      0.643272512125041,
      0.4734558421516133,
      0.4509703583881527,
      0.47125435344830247,
      0.5138659915702786,
      0.4774379330302427,
      0.4581024225957379,
      0.4470474353985872,
      0.5641403648489249,
      0.45616876486532704,
      0.49492236393654415,
      0.434584896712603,
      0.4615941649336301,
      0.42746296107412096,
      0.4880117024936362,
      0.4508083655805645,
      0.4569496808473222,
      0.4504201158970416,
      0.4677510437048124,
      0.47246236542384784,
      0.40261006499835833,
      0.41565634850970284,
      0.4496752548717453,
      0.3785603912469156,
      0.40090603484008125,
      0.40160598827157906,
      0.4230864807517229,
      0.40220797156502386,
      0.40192363986712015,
      0.4558534390465942,
      0.43123156585379274,
      0.43010145895138474,
      0.4181868948622378,
      0.3942737302737322,
      0.388541167617558,
      0.4136381569379818,
      0.3929855494620557,
      0.38053273887869843,
      0.41655928712940504,
      0.43266688529959696,
      0.3956322919644282,
      0.4150383190658992,
      0.42368077551890276,
      0.414082457419641,
      0.4423338885078887,
      0.42758217544969684,
      0.43120787003618516,
      0.45254075319288734,
      0.4821370544815492,
      0.4442744694784016,
      0.41068109541596054,
      0.45886389748778883,
      0.3906517159796047,
      0.44669128267351027,
      0.4119038478343073,
      0.393439493832474,
      0.40439956932367677,
      0.40604334933136754,
      0.4011363817724639,
      0.3821019156071954,
      0.3919581004691695,
      0.3675689763205494,
      0.3859400094715421,
      0.4089129067466645,
      0.4176033653363496,
      0.37652075758237324,
      0.401946436413034,
      0.45363395894002057,
      0.3883715167373954,
      0.37169236613961754,
      0.41569837101562296,
      0.4067961739030427,
      0.39417941752308144,
      0.40174060399660805,
      0.3950739826806291,
      0.3806714283492037,
      0.41237267265598215,
      0.39361231155738147,
      0.3803248889610439,
      0.3833808325364918,
      0.3983157638125791,
      0.37016333677097707,
      0.40749677047579586,
      0.40627464830518484,
      0.4148289348907813,
      0.4057087560435255,
      0.447360436323874,
      0.375728272651127,
      0.3855674023042896,
      0.353005801203722,
      0.41012082049946585,
      0.3970599546939313,
      0.3842919449338656,
      0.417644544383009,
      0.3920378703110946,
      0.39953203806263243,
      0.4346011827805799,
      0.3845181835982614,
      0.3940929396602208,
      0.4402445974107274,
      0.3848109642188706,
      0.3894334608833947,
      0.3934021602401476,
      0.38000216019189287,
      0.3977107434572574,
      0.4485347971409381,
      0.38416253490719254,
      0.4105494775368782,
      0.39874916482649875,
      0.38399478504043855,
      0.41863352568920503,
      0.37998146831453916,
      0.3893221895673318,
      0.42284767024531333,
      0.4221010481526038,
      0.39482483485501685,
      0.4031575343298341,
      0.41180295937075584,
      0.3912878304541468,
      0.40913491587260525,
      0.37541000770773003,
      0.39099362171338703,
      0.406419512807966,
      0.41070059171158396,
      0.39980611217593004,
      0.3972181370515309,
      0.36126206353931367,
      0.38473294179239675,
      0.37078485058810184,
      0.37028801042341186,
      0.37937556536433226,
      0.40784195206479396,
      0.3898714930532935,
      0.38368959259130286,
      0.37799677563284684,
      0.42646139119556564,
      0.3743629212329488,
      0.37048234925298634,
      0.3717771332420989,
      0.391420901767508
    ],
    "best_epoch": 100,
    "best_val_loss": 0.353005801203722,
    "test_loss": 0.4776940474812494,
    "tracker": {
      "initial_train_loss": 0.35503354064619436,
      "train_threshold": 0.11834451354873145,
      "best_tracking": true,
      "patience_active": true,
      "patience_best_train": null,
      "patience_best_val": 0.353005801203722,
      "patience_no_improve_epochs": 50,
      "best_train_loss": Infinity,
      "forced_stop_due_to_threshold": false
    }
  },
  "model_files": {
    "best": "scripts/outputs/mlp-other-value/trial_52_4cbb52/best_model.pt",
    "last": "scripts/outputs/mlp-other-value/trial_52_4cbb52/last_model.pt"
  },
  "config_path": "/home/amoris/data-analysis-on-dbps/scripts/outputs/mlp-other-value/trial_52_4cbb52/config.yaml"
}