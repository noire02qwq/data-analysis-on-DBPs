epoch,train_loss,val_loss
1,1.000334,0.357039
2,0.999577,0.354673
3,0.999388,0.354827
4,0.999485,0.353907
5,0.999376,0.352656
6,0.999506,0.348837
7,0.999588,0.350724
8,0.999453,0.348676
9,0.999422,0.352849
10,0.999276,0.352266
11,0.999513,0.356375
12,0.999528,0.356354
13,0.999469,0.354110
14,0.999494,0.355869
15,0.999425,0.355767
16,0.999602,0.351411
17,0.999344,0.353799
18,0.999473,0.349067
19,0.999470,0.352985
20,0.999262,0.352273
21,0.999297,0.351635
22,0.999496,0.352653
23,0.999422,0.350118
24,0.999312,0.355254
25,0.999496,0.351237
26,0.999246,0.354495
27,0.999290,0.349983
28,0.999295,0.353872
29,0.999312,0.352238
30,0.999293,0.354260
31,0.999385,0.353949
32,0.999286,0.353548
33,0.999297,0.352217
34,0.999424,0.352439
35,0.999460,0.352422
36,0.999353,0.350866
37,0.999295,0.351546
38,0.999266,0.354109
39,0.999350,0.352615
40,0.999334,0.351313
41,0.999306,0.352440
42,0.999251,0.351597
43,0.999245,0.352178
44,0.999249,0.351570
45,0.999334,0.350595
46,0.999461,0.355861
47,0.999561,0.351553
48,0.999256,0.352822
49,0.999280,0.353491
50,0.999282,0.352712
