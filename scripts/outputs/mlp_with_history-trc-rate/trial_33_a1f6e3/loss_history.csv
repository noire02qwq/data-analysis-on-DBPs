epoch,train_loss,val_loss
1,0.718491,0.243633
2,0.158785,0.253753
3,0.138063,0.276352
4,0.130904,0.307987
5,0.135166,0.271637
6,0.149817,0.375037
7,0.129051,0.501569
8,0.139600,0.267064
9,0.129107,0.287288
10,0.122827,0.272289
11,0.122199,0.691571
12,0.122134,0.343497
13,0.121055,0.264896
14,0.143234,0.283104
15,0.132094,0.371488
16,0.121478,0.368847
17,0.131593,0.231861
18,0.120766,0.273742
19,0.123376,0.384245
20,0.119141,0.303699
21,0.117869,0.422051
22,0.121445,0.313800
23,0.143637,0.312278
24,0.125902,0.567167
25,0.118729,0.325089
26,0.121856,0.314225
27,0.132354,0.266096
28,0.166736,0.288139
29,0.138086,0.277603
30,0.115992,0.273031
31,0.124058,0.378324
32,0.117950,0.279343
33,0.124953,0.278460
34,0.201232,0.294050
35,0.172443,0.342057
36,0.126282,0.345136
37,0.132106,0.486725
38,0.119727,0.331756
39,0.118189,0.288171
40,0.119418,0.300257
41,0.120022,0.310944
42,0.119777,0.281860
43,0.122778,0.308591
44,0.125016,0.284752
45,0.117308,0.289909
46,0.120039,0.932015
47,0.126356,0.300850
48,0.119905,0.278076
49,0.119505,0.349845
50,0.119069,0.332933
51,0.124856,0.385702
52,0.124373,0.355848
53,0.121775,0.311243
54,0.129729,0.288622
55,0.121115,0.444357
56,0.118279,0.274230
57,0.116701,0.476491
58,0.115185,0.287071
59,0.145565,0.286480
60,0.115717,0.683230
61,0.139035,0.276832
62,0.120761,0.571785
63,0.118642,0.479409
64,0.117648,0.736337
65,0.125485,0.295942
66,0.267783,0.280754
67,0.148160,0.294140
68,0.131988,0.272359
69,0.120215,0.434650
70,0.121510,0.271319
71,0.124936,0.268347
72,0.123775,0.284935
73,0.114992,0.402715
74,0.118259,0.286480
75,0.122448,0.289369
76,0.153570,0.268700
77,0.117868,0.270424
78,0.136583,0.289406
79,0.132453,0.368650
80,0.126304,0.336971
