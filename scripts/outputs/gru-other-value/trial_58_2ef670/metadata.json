{
  "model_name": "gru-other-value/trial_58_2ef670",
  "model_type": "GRU",
  "model_format": "torch",
  "model_params": {
    "history_length": 49,
    "units": 92,
    "num_layers": 2,
    "dropout": 0.3053624652394973
  },
  "training_params": {
    "max_epochs": 500,
    "batch_size": 124,
    "learning_rate": 0.0010174503914772422,
    "weight_decay": 0.0005929204210735684,
    "checkpoint_interval": 10,
    "seed": 947
  },
  "data_csv": "data/time_aligned_data.csv",
  "timestamp_column": "Date, Time",
  "columns": [
    "TRC-DT",
    "TRC-RT",
    "TRC-PPL1",
    "TRC-PPL2",
    "pH-DT",
    "pH-RT",
    "pH-PPL1",
    "pH-PPL2",
    "cond-DT",
    "cond-PPL1",
    "cond-PPL2",
    "fDOM-RT",
    "fDOM-PPL1",
    "fDOM-PPL2",
    "DO-RT",
    "DO-PPL1",
    "DO-PPL2",
    "TOC-RT",
    "TOC-PPL1",
    "TOC-PPL2",
    "DOC-RT",
    "DOC-PPL1",
    "DOC-PPL2",
    "minutes_since_start"
  ],
  "feature_columns": [
    "TRC-DT",
    "TRC-RT",
    "pH-DT",
    "pH-RT",
    "cond-DT",
    "fDOM-RT",
    "DO-RT",
    "TOC-RT",
    "DOC-RT",
    "minutes_since_start"
  ],
  "target_columns": [
    "TOC-PPL1",
    "TOC-PPL2",
    "DOC-PPL1",
    "DOC-PPL2",
    "pH-PPL1",
    "pH-PPL2"
  ],
  "split_boundaries": {
    "train_end": 7796,
    "val_end": 9466,
    "test_end": 11138
  },
  "dataset_sizes": {
    "train": 7748,
    "val": 1670,
    "test": 1672
  },
  "input_dim": 10,
  "sequence_length": 49,
  "training_history": {
    "epochs": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46,
      47,
      48,
      49,
      50,
      51,
      52,
      53,
      54,
      55,
      56,
      57,
      58,
      59,
      60,
      61,
      62,
      63,
      64,
      65,
      66,
      67,
      68,
      69,
      70,
      71,
      72,
      73,
      74,
      75,
      76,
      77,
      78,
      79,
      80,
      81,
      82,
      83,
      84,
      85,
      86,
      87,
      88,
      89,
      90,
      91,
      92,
      93,
      94,
      95,
      96,
      97,
      98,
      99,
      100,
      101,
      102,
      103,
      104,
      105,
      106,
      107,
      108,
      109,
      110,
      111,
      112,
      113,
      114,
      115,
      116,
      117,
      118,
      119,
      120,
      121,
      122,
      123,
      124,
      125
    ],
    "train_loss": [
      0.3733145458355477,
      0.23681765764751692,
      0.20480428404517562,
      0.17943954888575459,
      0.15988869375799758,
      0.14665608508844136,
      0.1356570906263993,
      0.12126155907291078,
      0.11658697766499478,
      0.11532003786001369,
      0.1082252423804509,
      0.0968343504677129,
      0.09644805848844654,
      0.08685585373707758,
      0.07704764475135319,
      0.08097297778073266,
      0.07312637591368154,
      0.07554305054028132,
      0.06683006355511077,
      0.061028638257460924,
      0.06051018989126154,
      0.05719888975163927,
      0.05684442639689379,
      0.05313186486341563,
      0.05689931052573945,
      0.057226771610910644,
      0.05019824677442667,
      0.05202999742000634,
      0.050051235745309494,
      0.05184972748568086,
      0.04817123657451134,
      0.048368398775190315,
      0.05058819045364149,
      0.04242391774510685,
      0.04789968851182531,
      0.04886699971782835,
      0.042740121141468426,
      0.04318467159330014,
      0.044888095854251235,
      0.039966684194434596,
      0.03943883570148641,
      0.0489618061807547,
      0.04244873487901675,
      0.039900796486965824,
      0.03729499377517984,
      0.038780687133568616,
      0.038251962293483696,
      0.04070033724806822,
      0.03937199750071835,
      0.0364008695800694,
      0.03918472062902132,
      0.03820442613243103,
      0.03749790433149979,
      0.03613218016033943,
      0.03451084590405918,
      0.04315716720767469,
      0.034844588546037183,
      0.03573376173994931,
      0.03549065903525378,
      0.03405103554961665,
      0.03845579361578639,
      0.03487733752016222,
      0.037484098170058405,
      0.0357452685539416,
      0.03495184676101452,
      0.03494985090644629,
      0.034316145983718384,
      0.03386402134454213,
      0.03170912410385011,
      0.0348659595133228,
      0.0362646552778792,
      0.0333098754311768,
      0.03400959442314347,
      0.03586686850574962,
      0.03567828857289434,
      0.03341107580678083,
      0.03346521849227155,
      0.04406388414947717,
      0.033293909434119515,
      0.032466180298513954,
      0.031446039312214784,
      0.03214265470175197,
      0.03249206494157254,
      0.03196341765945451,
      0.03204406426799285,
      0.031913251805017764,
      0.032783679492967795,
      0.03414181642480498,
      0.03166654318471083,
      0.035441480143304394,
      0.031243018622312525,
      0.03149934439108768,
      0.03069768278779261,
      0.031740593383060824,
      0.03602069633338306,
      0.03080014036809684,
      0.03358671649478887,
      0.034295276696873066,
      0.031246345182500922,
      0.03003467849917829,
      0.029240367234053228,
      0.029905427228053574,
      0.03502139494892365,
      0.03202088331254598,
      0.030228075323742755,
      0.033289584318232646,
      0.0294619804737675,
      0.029534319583009245,
      0.028713005093427803,
      0.029701377456997188,
      0.03325907937282284,
      0.05181162086424596,
      0.042162497995722276,
      0.030035424594818838,
      0.02934996589198978,
      0.029351596730613622,
      0.029662568909709884,
      0.03100880198988431,
      0.029370969236397104,
      0.028565957908608525,
      0.029540217188742276,
      0.03159199924115428,
      0.030617711467453974,
      0.030252208424540728,
      0.030120600771714745
    ],
    "val_loss": [
      0.38747909545541526,
      0.5608335487321465,
      0.616005990569463,
      0.6678572893410386,
      0.7336716884713687,
      0.7332093708529444,
      0.7705144516186799,
      0.8816993226548155,
      0.8668115448095127,
      0.6898831622150844,
      0.7042564844299933,
      0.6022703020515556,
      0.8123258315695974,
      0.7063650896092375,
      0.5841070886500581,
      0.5711628416698136,
      0.574657730642193,
      0.6245274737834217,
      0.5527499349442071,
      0.5723803034977999,
      0.6306163764463927,
      0.6318044121572357,
      0.5295331566455121,
      0.6281128467646188,
      0.4999468532329548,
      0.6010876617388811,
      0.6096144553430066,
      0.6456997389743429,
      0.5488748588337156,
      0.5502347857027711,
      0.49935734865372766,
      0.47407649883074676,
      0.5493579129972858,
      0.600318462647007,
      0.4843938470779065,
      0.65780050516307,
      0.4520174170504073,
      0.5201068281948923,
      0.5696318604542824,
      0.5450848332511451,
      0.5234029912306163,
      0.5130698877269636,
      0.5287974291308197,
      0.4736764296532391,
      0.5434709769255387,
      0.538993679183329,
      0.4770043387919843,
      0.490444629019249,
      0.4028167714526553,
      0.42087325782119156,
      0.4022657986529573,
      0.4332018158036078,
      0.44951813625182935,
      0.47459451019942406,
      0.4539047461694586,
      0.4590215257898776,
      0.442017040861224,
      0.4328547818575077,
      0.432520072483374,
      0.4181077653954843,
      0.43363241426602095,
      0.450378392636776,
      0.4441988218062652,
      0.4345782319139578,
      0.463647100034945,
      0.43488647167732614,
      0.4421714940827764,
      0.452176446855782,
      0.44049410687056845,
      0.4367060550047966,
      0.4515670160779696,
      0.4365277854982251,
      0.41427753747937207,
      0.4121172152950378,
      0.3970362430293403,
      0.4159847526760872,
      0.47565967436143736,
      0.43386571940904606,
      0.4407868403606786,
      0.44030206236475244,
      0.43805477079338656,
      0.4575736153000843,
      0.4541218728808586,
      0.4621113698014956,
      0.44681297413960186,
      0.44873067005844175,
      0.4626960141573124,
      0.43158721471439576,
      0.45330048129408657,
      0.43553480066939027,
      0.4510726932993906,
      0.43206810697823944,
      0.4529172375352083,
      0.4598946061141477,
      0.4582211172152422,
      0.4537663982924587,
      0.44758229092388097,
      0.45789583833988556,
      0.4549606398729507,
      0.4544561330875951,
      0.43898535186123705,
      0.43360762292753435,
      0.43669148174945466,
      0.44975322340419904,
      0.4474073869917921,
      0.4402698253889284,
      0.4685119582150511,
      0.45892825034742585,
      0.45540103269908244,
      0.4449512146368712,
      0.508906319344829,
      0.45354431022605496,
      0.4545509492565772,
      0.44121770808796684,
      0.46642116142247253,
      0.46904603097431674,
      0.46908427329655894,
      0.47798037959965406,
      0.4383308963682837,
      0.4352507443128232,
      0.45271146448251015,
      0.46459099357713485,
      0.47232965306786007,
      0.4568029258154823,
      0.47717902767444087
    ],
    "best_epoch": 75,
    "best_val_loss": 0.3970362430293403,
    "test_loss": 0.35914298751208773,
    "tracker": {
      "initial_train_loss": 0.3733145458355477,
      "train_threshold": 0.12443818194518257,
      "best_tracking": true,
      "patience_active": true,
      "patience_best_train": null,
      "patience_best_val": 0.3970362430293403,
      "patience_no_improve_epochs": 50,
      "best_train_loss": Infinity,
      "forced_stop_due_to_threshold": false
    }
  },
  "model_files": {
    "best": "scripts/outputs/gru-other-value/trial_58_2ef670/best_model.pt",
    "last": "scripts/outputs/gru-other-value/trial_58_2ef670/last_model.pt"
  },
  "config_path": "/home/amoris/data-analysis-on-dbps/scripts/outputs/gru-other-value/trial_58_2ef670/config.yaml"
}