{
  "model_name": "mlp_regressor",
  "model_type": "MLP",
  "model_params": {
    "history_length": 48,
    "hidden_layers": [
      384,
      192,
      128
    ],
    "dropout": 0.1
  },
  "input_dim": 1170,
  "output_dim": 6,
  "columns": [
    "TRC-DT",
    "TRC-RT",
    "TRC-PPL1",
    "TRC-PPL2",
    "pH-DT",
    "pH-RT",
    "pH-PPL1",
    "pH-PPL2",
    "cond-DT",
    "cond-PPL1",
    "cond-PPL2",
    "fDOM-RT",
    "fDOM-PPL1",
    "fDOM-PPL2",
    "DO-RT",
    "DO-PPL1",
    "DO-PPL2",
    "TOC-RT",
    "TOC-PPL1",
    "TOC-PPL2",
    "DOC-RT",
    "DOC-PPL1",
    "DOC-PPL2",
    "minutes_since_start"
  ],
  "non_target_columns": [
    "TRC-DT",
    "TRC-RT",
    "TRC-PPL1",
    "TRC-PPL2",
    "pH-DT",
    "pH-RT",
    "pH-PPL1",
    "pH-PPL2",
    "cond-DT",
    "cond-PPL1",
    "cond-PPL2",
    "fDOM-RT",
    "fDOM-PPL1",
    "fDOM-PPL2",
    "DO-RT",
    "TOC-RT",
    "DOC-RT",
    "minutes_since_start"
  ],
  "target_columns": [
    "DO-PPL1",
    "DO-PPL2",
    "TOC-PPL1",
    "TOC-PPL2",
    "DOC-PPL1",
    "DOC-PPL2"
  ],
  "data_csv": "data/time_aligned_data.csv",
  "timestamp_column": "Date, Time",
  "split_boundaries": {
    "train_end": 8250,
    "val_end": 10018,
    "test_end": 11787
  },
  "dataset_sizes": {
    "train": 8202,
    "val": 1768,
    "test": 1769
  },
  "training_history": {
    "epochs": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46,
      47,
      48,
      49,
      50,
      51,
      52,
      53,
      54,
      55,
      56,
      57,
      58,
      59,
      60,
      61,
      62,
      63,
      64,
      65,
      66,
      67,
      68,
      69,
      70,
      71,
      72,
      73,
      74,
      75,
      76,
      77,
      78,
      79,
      80,
      81,
      82,
      83,
      84,
      85,
      86,
      87,
      88,
      89,
      90,
      91,
      92,
      93,
      94,
      95,
      96,
      97,
      98,
      99,
      100,
      101,
      102,
      103,
      104,
      105,
      106,
      107,
      108,
      109,
      110,
      111,
      112,
      113,
      114,
      115,
      116,
      117,
      118,
      119,
      120
    ],
    "train_loss": [
      0.15832899929353797,
      0.049985220630818304,
      0.04249803413507765,
      0.038856539847108976,
      0.03690473647264483,
      0.03747904913395941,
      0.03318844346742431,
      0.03248068467382104,
      0.03331317748330303,
      0.03358918791502838,
      0.029276157365292058,
      0.02977765815991566,
      0.028497494935673263,
      0.028856681122961667,
      0.028000936856399337,
      0.02877582919730977,
      0.02820190206402629,
      0.026562042448508887,
      0.02636638944534827,
      0.027382148215826532,
      0.026507128284409754,
      0.02760193544905292,
      0.02751449524622317,
      0.02618786560948797,
      0.024177978681416315,
      0.03001225228160002,
      0.025526355817583356,
      0.026492508116930585,
      0.024942898965229263,
      0.02409030171372387,
      0.024588543517477364,
      0.024049820189024292,
      0.024821885932537587,
      0.024172560138125094,
      0.025166915641074296,
      0.024300474539068494,
      0.024823962471211023,
      0.026404293550605803,
      0.024020277108598324,
      0.025889134778482426,
      0.023632017641231306,
      0.02494142196211356,
      0.024329972084837843,
      0.02287790130108951,
      0.023529071915850235,
      0.02508123447790186,
      0.02346253411204415,
      0.023525731593563016,
      0.022826040009210617,
      0.022259415625359512,
      0.022594429454449415,
      0.025417878489166504,
      0.024403716410039925,
      0.02278126429487542,
      0.02307322575244939,
      0.022044322051251408,
      0.023872852949954712,
      0.02292658659813245,
      0.022862598290442113,
      0.023321366720619185,
      0.02364780616185064,
      0.023634138595430017,
      0.022257905638300043,
      0.023265332863792156,
      0.02279736033089639,
      0.02274908208699524,
      0.02201342057246852,
      0.022732851826748203,
      0.022811470000980338,
      0.023204810264248057,
      0.022022525628429598,
      0.021415678398641426,
      0.023445116741170797,
      0.021580535539223112,
      0.022915514249611673,
      0.022863621176037664,
      0.02176898149230718,
      0.020925347100769827,
      0.021458482701341368,
      0.022481994005317543,
      0.022362017482646324,
      0.021532129279685962,
      0.020889293749436783,
      0.021922692821389058,
      0.02222437309035366,
      0.020001784969876087,
      0.02005977522203927,
      0.021753821219050777,
      0.020895175926498134,
      0.021306730299494057,
      0.021704032702402818,
      0.02077695208932533,
      0.022398124607695483,
      0.020536132884011765,
      0.021285251240793075,
      0.021103058165219523,
      0.02123532304460793,
      0.02057512165736466,
      0.0199843920491158,
      0.02089512518270296,
      0.020439249345970516,
      0.021062216117625644,
      0.02056007566267951,
      0.024880830940223416,
      0.020925026270248663,
      0.02182904700345526,
      0.021072442652655357,
      0.022045406989137362,
      0.020407488617307526,
      0.021618285599957005,
      0.020156218852667045,
      0.02022209416614367,
      0.019987293940343615,
      0.02057918123796426,
      0.019635661113421327,
      0.02017648324690136,
      0.020403664626346452,
      0.02071585047284268,
      0.02200516368453458,
      0.020635868095441056
    ],
    "val_loss": [
      0.09811959877542781,
      0.08304622418740217,
      0.08229382754181305,
      0.07817151254186264,
      0.12326427120968228,
      0.0618023280711735,
      0.04695764448645428,
      0.046213824369384156,
      0.11454077179615314,
      0.063734872814487,
      0.05727291370139403,
      0.03191525063464814,
      0.040020321171343055,
      0.044970154206003,
      0.041134052164009796,
      0.04942430579419589,
      0.0451643605341469,
      0.03349005577588513,
      0.03473713468102848,
      0.0299404287095523,
      0.07430463958514762,
      0.055704922613380184,
      0.06178036743429451,
      0.025882523683873237,
      0.03486027153908397,
      0.03963616111092438,
      0.061665527084294486,
      0.049183439193672726,
      0.040231882548170395,
      0.05077246433267227,
      0.04873921939622763,
      0.052932393631784085,
      0.0528199857446403,
      0.033003220461072964,
      0.037208015444726426,
      0.0524131434872679,
      0.07464046975201612,
      0.05366218287278624,
      0.03528422665070085,
      0.04039535633545386,
      0.031996878968105054,
      0.06045217110830195,
      0.03843741024511432,
      0.02562681677418327,
      0.047658772321578065,
      0.09553768873484426,
      0.0806920334899048,
      0.057490004605836996,
      0.02884056348336768,
      0.04014986726493318,
      0.20612162446004773,
      0.047792449762109175,
      0.02872165193890824,
      0.051670782065769125,
      0.04043158416834352,
      0.07024145104429301,
      0.03909098997509857,
      0.042845995961878096,
      0.0716477082325862,
      0.038277749090173126,
      0.0369583858573194,
      0.04319886537900877,
      0.09179541148346473,
      0.03292597088608806,
      0.0559112712639759,
      0.09354691301805401,
      0.029454019721344586,
      0.03669825168452921,
      0.030570571694303963,
      0.04464353624377315,
      0.03615387953918983,
      0.03626435223073442,
      0.031150098746785752,
      0.04715130902927925,
      0.03308256868930424,
      0.033388751748595305,
      0.033697556687426244,
      0.035535124455035004,
      0.09396119918084253,
      0.04735639087889529,
      0.04702460153104223,
      0.03010614271954174,
      0.03859961289086493,
      0.03270938839105999,
      0.04651711898017253,
      0.0509910698036128,
      0.041417362800550674,
      0.042999566208183494,
      0.044326731252454524,
      0.053599238597969126,
      0.04416885757945242,
      0.06573369016879285,
      0.03892688779371087,
      0.07262065049210285,
      0.0335932884163042,
      0.07776296947876253,
      0.10234081660865119,
      0.034048217352968535,
      0.04603363753073086,
      0.05636106622805692,
      0.02862995302481619,
      0.034644155463886474,
      0.09358761307880349,
      0.0413729416889168,
      0.03232796630944332,
      0.06415071171535626,
      0.04414186360339773,
      0.03672305895731999,
      0.05701224431732661,
      0.04130727290365491,
      0.04330433610135614,
      0.03550269846631661,
      0.034553981946604286,
      0.04314384778026002,
      0.04995919612333246,
      0.03047933758181684,
      0.0418072375574263,
      0.04691875823752373,
      0.047446284224005306,
      0.03208724013213658
    ],
    "best_epoch": 44,
    "best_val_loss": 0.02562681677418327,
    "test_loss": 0.04868855551011553,
    "train_threshold": 0.03958224982338449,
    "tracking_enabled": true,
    "initial_train_loss": 0.15832899929353797
  },
  "config_path": "models/configs/mlp_config.yaml"
}