{
  "model_name": "lstm_regressor/0048",
  "model_type": "LSTM",
  "model_format": "torch",
  "model_params": {
    "history_length": 80,
    "units": 192,
    "num_layers": 4,
    "dropout": 0.35,
    "fc_dim": null
  },
  "training_params": {
    "max_epochs": 500,
    "batch_size": 224,
    "learning_rate": 0.0008,
    "weight_decay": 0.0,
    "checkpoint_interval": 10,
    "seed": 947
  },
  "data_csv": "data/time_aligned_data.csv",
  "timestamp_column": "Date, Time",
  "columns": [
    "TRC-DT",
    "TRC-RT",
    "TRC-PPL1",
    "TRC-PPL2",
    "pH-DT",
    "pH-RT",
    "pH-PPL1",
    "pH-PPL2",
    "cond-DT",
    "cond-PPL1",
    "cond-PPL2",
    "fDOM-RT",
    "fDOM-PPL1",
    "fDOM-PPL2",
    "DO-RT",
    "DO-PPL1",
    "DO-PPL2",
    "TOC-RT",
    "TOC-PPL1",
    "TOC-PPL2",
    "DOC-RT",
    "DOC-PPL1",
    "DOC-PPL2",
    "minutes_since_start"
  ],
  "feature_columns": [
    "TRC-DT",
    "TRC-RT",
    "pH-DT",
    "pH-RT",
    "cond-DT",
    "fDOM-RT",
    "DO-RT",
    "TOC-RT",
    "DOC-RT",
    "minutes_since_start"
  ],
  "target_columns": [
    "TRC-PPL1",
    "TRC-PPL2",
    "TOC-PPL1",
    "TOC-PPL2",
    "DOC-PPL1",
    "DOC-PPL2",
    "pH-PPL1",
    "pH-PPL2"
  ],
  "split_boundaries": {
    "train_end": 7796,
    "val_end": 9466,
    "test_end": 11138
  },
  "dataset_sizes": {
    "train": 7717,
    "val": 1670,
    "test": 1672
  },
  "input_dim": 10,
  "sequence_length": 80,
  "training_history": {
    "epochs": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46,
      47,
      48,
      49,
      50,
      51,
      52,
      53,
      54,
      55,
      56,
      57,
      58,
      59,
      60,
      61,
      62,
      63,
      64,
      65,
      66,
      67,
      68,
      69,
      70,
      71,
      72,
      73,
      74,
      75,
      76,
      77,
      78,
      79,
      80,
      81,
      82,
      83,
      84,
      85,
      86,
      87,
      88,
      89,
      90,
      91,
      92,
      93,
      94,
      95,
      96,
      97,
      98,
      99,
      100,
      101,
      102,
      103,
      104,
      105,
      106,
      107,
      108,
      109,
      110,
      111,
      112,
      113,
      114,
      115,
      116,
      117,
      118,
      119,
      120,
      121,
      122,
      123,
      124,
      125,
      126,
      127,
      128,
      129,
      130,
      131,
      132,
      133,
      134,
      135,
      136,
      137,
      138,
      139,
      140,
      141,
      142,
      143,
      144,
      145,
      146,
      147,
      148,
      149,
      150,
      151,
      152,
      153,
      154,
      155,
      156,
      157,
      158,
      159,
      160,
      161,
      162,
      163,
      164,
      165,
      166,
      167,
      168,
      169,
      170,
      171,
      172,
      173,
      174,
      175,
      176,
      177,
      178,
      179,
      180,
      181,
      182,
      183,
      184,
      185,
      186,
      187,
      188,
      189,
      190,
      191,
      192,
      193
    ],
    "train_loss": [
      0.5714499248507666,
      0.2943025360426261,
      0.2496495466610303,
      0.19487888115249768,
      0.1557342472264259,
      0.16509056436756014,
      0.14346222870859965,
      0.11431859329862268,
      0.09182266642006318,
      0.08990116730029216,
      0.09517901757575385,
      0.07704242094733314,
      0.06210249079010238,
      0.05497947574102061,
      0.05701879498683852,
      0.054715328162795014,
      0.060466160981093905,
      0.052457197398542044,
      0.044875559279133004,
      0.03966189704539014,
      0.040708095366293336,
      0.03766566609489002,
      0.03650563623376255,
      0.038724547702695046,
      0.031072834622729424,
      0.035511444962996346,
      0.031831860174933634,
      0.02830247379751562,
      0.026331054799338845,
      0.025758093915222238,
      0.026912870708632394,
      0.025579990032474752,
      0.02412506670374759,
      0.023116086985593913,
      0.023265470070813903,
      0.022625451089032994,
      0.02311222241447776,
      0.022907965510863475,
      0.02205000895858622,
      0.021824393087115347,
      0.02100271002125312,
      0.02097089047492655,
      0.0205791957316288,
      0.02024085382263911,
      0.020278548383722037,
      0.01979680605584149,
      0.026058474653607454,
      0.03239076111397626,
      0.1127588210688503,
      0.08616117917573775,
      0.05407216371812706,
      0.03649774219831593,
      0.02885623256417272,
      0.02576374711204034,
      0.02542969917288844,
      0.025157323469659727,
      0.02425301038269453,
      0.02266167042139969,
      0.022416752762730464,
      0.02128544437399843,
      0.020697502157225715,
      0.02036153168153335,
      0.02034922369626793,
      0.019806140540893004,
      0.02026353664725181,
      0.01996123715506226,
      0.019576708798137325,
      0.02040878245485127,
      0.02470678272679423,
      0.02360957315307725,
      0.021766802049604617,
      0.020420047103223274,
      0.020696849366598092,
      0.01985350992538791,
      0.019047703329911576,
      0.018945211784553773,
      0.018441563588250283,
      0.018216818367857938,
      0.021414048158986915,
      0.019525850195815653,
      0.018120278962641023,
      0.017518654645495315,
      0.017397629542430015,
      0.01724437894513238,
      0.017099545667814717,
      0.016888647777598857,
      0.016687830332973174,
      0.016833092710318243,
      0.01668129596500542,
      0.016581462625956247,
      0.016426788962092428,
      0.01628924541762375,
      0.016458103158814744,
      0.016277997138661023,
      0.01617493901246799,
      0.015823588402065894,
      0.015862166961213075,
      0.015938428113036928,
      0.01580225739054101,
      0.016387670608525823,
      0.01677980679582751,
      0.0158321304690661,
      0.015374848017370329,
      0.015243606332815518,
      0.015156270967750827,
      0.015939245217231033,
      0.015914754439046365,
      0.02550862242804532,
      0.028666724544899505,
      0.02438664930881429,
      0.02002716203216206,
      0.018001961812533385,
      0.016741841939034528,
      0.016271953050177982,
      0.015539703007029084,
      0.015377165428165468,
      0.015284822937798682,
      0.015070307501904918,
      0.014925706031946718,
      0.014806685051427607,
      0.014976251215559453,
      0.01464067220961096,
      0.014662665729765155,
      0.01448330124078621,
      0.014490047052045962,
      0.014364540206191916,
      0.01434152354690847,
      0.014207495933426845,
      0.01414718221968627,
      0.015343915473549566,
      0.015124710230259361,
      0.014261916793667546,
      0.013989557300785887,
      0.013751048741103555,
      0.013832139806261724,
      0.013582121713230094,
      0.013484502851203213,
      0.013518344297994483,
      0.01333472321493759,
      0.013436392016995598,
      0.013421316629810795,
      0.013352394644549948,
      0.013270661265750893,
      0.013472052559800116,
      0.02248854360710822,
      0.07668255083480152,
      0.07143084459258775,
      0.09372555927452035,
      0.07535297877809476,
      0.041321474611295754,
      0.03159761947245506,
      0.026057791107973703,
      0.023681941011227118,
      0.023454647990602803,
      0.021704192421881433,
      0.020038146459969024,
      0.01907707958905688,
      0.018700239269210835,
      0.018091190058447868,
      0.01787020860427407,
      0.017676035602655113,
      0.017108854705324687,
      0.01686269632199468,
      0.016732653455239853,
      0.01657115346578925,
      0.016405941882286956,
      0.016182427442752938,
      0.015947743981412396,
      0.015885595720779314,
      0.015724490125017553,
      0.015615121431159234,
      0.015681510516866566,
      0.015376309025344785,
      0.015236081043541919,
      0.015059485156569392,
      0.014829871258003864,
      0.015035747340074766,
      0.02171851586764327,
      0.016778638251140295,
      0.016680050103045258,
      0.2593479409371693,
      0.1772866340727483,
      0.12089446058417905,
      0.12209169025288756,
      0.09495429136896899,
      0.07571763397943052,
      0.06887603640262831,
      0.06122651233500623,
      0.05969396670147037,
      0.059392055705840434,
      0.05017493133047817,
      0.05429199907155841,
      0.044649398151543285
    ],
    "val_loss": [
      1.0294420088122704,
      0.578522952016956,
      0.6009995457654942,
      0.7198561077346345,
      0.6751101214014842,
      0.5110814498570151,
      0.47712370921037867,
      0.45549707469825973,
      0.48275930881500245,
      0.5158763861227892,
      0.48027662545621036,
      0.5228252673577406,
      0.4808542198763636,
      0.4817529559849265,
      0.4968168194422465,
      0.4701871185245628,
      0.4953501584287175,
      0.47439208244849107,
      0.5331017322169093,
      0.4926572027320633,
      0.5022789671749412,
      0.5172804048675262,
      0.4852290479722851,
      0.4652022971364552,
      0.472967589187051,
      0.4376353514408637,
      0.47042332259480824,
      0.46263038084178626,
      0.4515148760315901,
      0.4568341743446396,
      0.4569557927325814,
      0.4567589417189181,
      0.4561659378919773,
      0.46033101917027,
      0.44367862497261185,
      0.4425979945474042,
      0.43730997411076894,
      0.4630506525496523,
      0.4365814031955011,
      0.4439408574989456,
      0.4494598547855537,
      0.44501284575034045,
      0.44288183186582464,
      0.4413735393992441,
      0.43747212808289215,
      0.4512366787402216,
      0.4587855689539881,
      0.5108167495556221,
      0.4428992679733002,
      0.45156281436988693,
      0.4754322366086309,
      0.4389054607488438,
      0.4396937844995967,
      0.40375659908363204,
      0.4131718681957907,
      0.401526079420558,
      0.4121014923392656,
      0.40699391821901243,
      0.39662256312227534,
      0.3985270307449524,
      0.4114162480759763,
      0.4240867451279463,
      0.418251982206356,
      0.4109408859007373,
      0.40058189751859197,
      0.42550483206788936,
      0.4016180034883008,
      0.4175250440300582,
      0.493494238253839,
      0.46803453097086467,
      0.4734321074571438,
      0.45611278668135224,
      0.4506812438279569,
      0.4506162396448101,
      0.4556370169816617,
      0.44069119356349556,
      0.4695385257640999,
      0.4493563086686734,
      0.4716675524226206,
      0.4579422316865293,
      0.45974537084202566,
      0.4748462699844452,
      0.4617613973731766,
      0.4458596308074312,
      0.45397054492356537,
      0.45908901762819576,
      0.4654264581417609,
      0.46661258172132297,
      0.4536319392883849,
      0.45170079305500327,
      0.4496147801062304,
      0.4642251026130722,
      0.45375473427915286,
      0.45958695968468033,
      0.47038159484634856,
      0.4613786860140498,
      0.45687088780774326,
      0.45870702980521194,
      0.45961081510532403,
      0.4746358404616396,
      0.44246722424101687,
      0.45913210314904856,
      0.4447433194714392,
      0.4606593737345256,
      0.480152014486804,
      0.45785186390676896,
      0.47332737759915655,
      0.488636721108488,
      0.4795903388611571,
      0.45171764653599905,
      0.48069064974071024,
      0.4589555175004605,
      0.4785253900253844,
      0.45873668108157767,
      0.4740930110394598,
      0.46865263827546627,
      0.4761743802510336,
      0.47062500608181523,
      0.46817948389909936,
      0.4746413916170954,
      0.4795159733937886,
      0.47632928322889134,
      0.48165610250598656,
      0.4903805038886156,
      0.4939710105964524,
      0.4835659435409272,
      0.4913574889748396,
      0.49421173341259983,
      0.47883870787249355,
      0.4777898070341099,
      0.4864810746586965,
      0.47807393245354385,
      0.4797934009643372,
      0.4902849355857529,
      0.4969564370766371,
      0.48376716482425164,
      0.5023904412092562,
      0.5011252657382074,
      0.4889902363280336,
      0.49877194358917054,
      0.4819865557961835,
      0.495388309398811,
      0.4762676024865248,
      0.5058658839699751,
      0.49822267829301115,
      0.5126054715253636,
      0.4857681188754693,
      0.4783877469822318,
      0.5353013399832263,
      0.4630920427288124,
      0.48811980478777856,
      0.4854047661056062,
      0.48499591322002295,
      0.4914030843152257,
      0.496520636752694,
      0.4901092679200772,
      0.4833852732252932,
      0.4859299185747158,
      0.49279211524003996,
      0.4926221684781377,
      0.48675100632056506,
      0.49228547847199583,
      0.4875765299368761,
      0.48884440296424364,
      0.4967484431352444,
      0.4887261430660408,
      0.49640894521496254,
      0.49895955502629996,
      0.48946053010974816,
      0.4948213937039861,
      0.49889134047274103,
      0.4924130571102668,
      0.5011256307898881,
      0.49448056092519244,
      0.4854337525224971,
      0.48716302503368814,
      0.489995885894684,
      0.5034226300473699,
      0.49998374941820156,
      0.5866210195118796,
      0.5139194121617757,
      0.4302953234689678,
      0.4304444003247929,
      0.44824692874611494,
      0.44578527947385865,
      0.48446313378339756,
      0.49799984286645216,
      0.464625457518115,
      0.4999729280700227,
      0.4852904428265052,
      0.5332772360590403,
      0.48387032226174176,
      0.4718601054060245
    ],
    "best_epoch": 59,
    "best_val_loss": 0.39662256312227534,
    "test_loss": 2.135229221371372,
    "tracker": {
      "initial_train_loss": 0.5714499248507666,
      "train_threshold": 0.19048330828358886,
      "best_tracking": true,
      "patience_active": true,
      "patience_best_train": 0.013270661265750893,
      "patience_best_val": 0.39662256312227534,
      "patience_no_improve_epochs": 50,
      "best_train_loss": 0.013270661265750893,
      "forced_stop_due_to_threshold": false
    }
  },
  "model_files": {
    "best": "scripts/outputs/lstm_regressor/0048/best_model.pt",
    "last": "scripts/outputs/lstm_regressor/0048/last_model.pt"
  },
  "config_path": "/home/amoris/data-analysis-on-dbps/scripts/outputs/lstm_regressor/tmp_dropout_b5ffd1/config.yaml"
}