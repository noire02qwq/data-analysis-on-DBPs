{
  "model_name": "mlp-other-value/trial_61_19ac5b",
  "model_type": "MLP",
  "model_format": "torch",
  "model_params": {
    "history_length": 1,
    "hidden_layers": [
      512,
      256,
      128
    ],
    "dropout": 0.30738915887824925,
    "mid_layer_count": 8,
    "mid_layer_size": 484
  },
  "training_params": {
    "max_epochs": 400,
    "batch_size": 186,
    "learning_rate": 0.0013688428420523201,
    "weight_decay": 0.0006378680094533302,
    "checkpoint_interval": 10,
    "seed": 42
  },
  "data_csv": "data/time_aligned_data.csv",
  "timestamp_column": "Date, Time",
  "columns": [
    "TRC-DT",
    "TRC-RT",
    "TRC-PPL1",
    "TRC-PPL2",
    "pH-DT",
    "pH-RT",
    "pH-PPL1",
    "pH-PPL2",
    "cond-DT",
    "cond-PPL1",
    "cond-PPL2",
    "fDOM-RT",
    "fDOM-PPL1",
    "fDOM-PPL2",
    "DO-RT",
    "DO-PPL1",
    "DO-PPL2",
    "TOC-RT",
    "TOC-PPL1",
    "TOC-PPL2",
    "DOC-RT",
    "DOC-PPL1",
    "DOC-PPL2",
    "minutes_since_start"
  ],
  "feature_columns": [
    "TRC-DT",
    "TRC-RT",
    "pH-DT",
    "pH-RT",
    "cond-DT",
    "fDOM-RT",
    "DO-RT",
    "TOC-RT",
    "DOC-RT",
    "minutes_since_start"
  ],
  "target_columns": [
    "TOC-PPL1",
    "TOC-PPL2",
    "DOC-PPL1",
    "DOC-PPL2",
    "pH-PPL1",
    "pH-PPL2"
  ],
  "split_boundaries": {
    "train_end": 7796,
    "val_end": 9466,
    "test_end": 11138
  },
  "dataset_sizes": {
    "train": 7796,
    "val": 1670,
    "test": 1672
  },
  "input_dim": 10,
  "sequence_length": null,
  "training_history": {
    "epochs": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46,
      47,
      48,
      49,
      50,
      51,
      52,
      53,
      54,
      55,
      56,
      57,
      58,
      59,
      60,
      61,
      62,
      63,
      64,
      65,
      66,
      67,
      68,
      69,
      70,
      71,
      72,
      73,
      74,
      75,
      76,
      77,
      78,
      79,
      80,
      81,
      82,
      83,
      84,
      85,
      86,
      87,
      88,
      89,
      90,
      91,
      92,
      93,
      94,
      95,
      96,
      97,
      98,
      99,
      100,
      101,
      102,
      103,
      104,
      105,
      106,
      107,
      108,
      109,
      110,
      111,
      112,
      113,
      114,
      115,
      116,
      117,
      118,
      119,
      120,
      121,
      122,
      123,
      124,
      125,
      126,
      127,
      128,
      129,
      130,
      131,
      132,
      133,
      134,
      135,
      136,
      137,
      138,
      139,
      140,
      141,
      142,
      143,
      144,
      145,
      146,
      147,
      148,
      149,
      150
    ],
    "train_loss": [
      0.38354347178329257,
      0.25113912602612276,
      0.2249692063520602,
      0.21073637192848096,
      0.2037822123351251,
      0.19513134268796278,
      0.18799788387479874,
      0.18035398901518093,
      0.17612397026608942,
      0.1705471674155247,
      0.16787230814417672,
      0.16523619053867183,
      0.15969178770760073,
      0.15821746727590258,
      0.1552435560691901,
      0.1539089727464427,
      0.150767265393252,
      0.1481076168702039,
      0.14756199281343502,
      0.1466888863346771,
      0.14322840613339302,
      0.14246622675417203,
      0.14170249176520822,
      0.14020138973223972,
      0.13921016481264914,
      0.135953655759835,
      0.13577885282588592,
      0.13419734533893382,
      0.1329829179901169,
      0.13156298073959571,
      0.13054620695533112,
      0.1289176658415195,
      0.13017473299880097,
      0.12738120670178538,
      0.12809086700145436,
      0.12686797137777886,
      0.12697380679679138,
      0.12669647635443815,
      0.1272504534696603,
      0.12568279533714377,
      0.12177489162171848,
      0.12221407966584105,
      0.1232280770938876,
      0.12213821360328186,
      0.12061297073539799,
      0.11977570153975438,
      0.12012013569741814,
      0.12214000136034803,
      0.11930243899350963,
      0.11836741962353042,
      0.12247820438223658,
      0.11782811842228035,
      0.11654800215421608,
      0.11759434664873296,
      0.11630888337918341,
      0.11662977192648379,
      0.11695011387296736,
      0.11660149823463287,
      0.11599004155301082,
      0.11514027520494317,
      0.11500046269183528,
      0.11514291233775431,
      0.11322548575871354,
      0.11448814787387297,
      0.1161306613569782,
      0.11310408400680359,
      0.11278071928828604,
      0.1133893456266072,
      0.11277062540874841,
      0.11303808098575774,
      0.1112460936297815,
      0.11182986610803683,
      0.11342813478655665,
      0.11155321150100678,
      0.1140755539565988,
      0.11013025328853364,
      0.11325048363594106,
      0.11181548475385814,
      0.11089361470102836,
      0.11099883733750185,
      0.10808663263634084,
      0.11191483545358392,
      0.11007415360100702,
      0.1109024315839275,
      0.110712356290079,
      0.11008898728647068,
      0.10917152956953839,
      0.10805831486411802,
      0.10817802115403242,
      0.10888731690470593,
      0.1092300398906969,
      0.11024243939666029,
      0.10885195787317511,
      0.10684956334389864,
      0.10589824331730313,
      0.11010001403489376,
      0.10812401484837589,
      0.10694270348200252,
      0.10774324385935738,
      0.10981444460748403,
      0.10752717298137769,
      0.10844248874431209,
      0.10700125092226705,
      0.10761926022705633,
      0.1062511618704506,
      0.10808922383091338,
      0.10872411355376244,
      0.10540484276343579,
      0.10575367550840128,
      0.10654516147498352,
      0.10624251905360425,
      0.10621915556769483,
      0.1075682858260336,
      0.10587342231586507,
      0.10529686042082316,
      0.10552218742190782,
      0.10619699215853808,
      0.10537766432413509,
      0.10529062054237076,
      0.10457048673716002,
      0.10432861778424482,
      0.10670081024111204,
      0.10431543230750488,
      0.1067624528681608,
      0.1050159091434765,
      0.10534822440999113,
      0.10586756598940257,
      0.10276362058893419,
      0.10698541008000988,
      0.10547441467757468,
      0.10499235090458925,
      0.10398221572907905,
      0.10571433772538918,
      0.10429595147052625,
      0.10350254088020924,
      0.10525276634618526,
      0.10585583285024804,
      0.10440620296601762,
      0.10507418966640443,
      0.10369584502402851,
      0.10708627918244876,
      0.10522035656906875,
      0.10437473523243689,
      0.10402046028178677,
      0.10244188951438302,
      0.1042332102058636,
      0.10473247239219953,
      0.10467831208430357,
      0.1045455603877999,
      0.10242267340429322
    ],
    "val_loss": [
      0.3626966900008167,
      0.48422759133363197,
      0.5018828128893932,
      0.4849606938704759,
      0.491584648183006,
      0.4879992699194811,
      0.5130399462616372,
      0.4536394353398306,
      0.4899052882265902,
      0.5476152221688967,
      0.5521754403849561,
      0.45678812866796276,
      0.4604933350118334,
      0.48509031668929997,
      0.5245635504137256,
      0.4815968928133656,
      0.45094792155805463,
      0.4966224955316789,
      0.5235380966863232,
      0.41768516167373715,
      0.46714721581714597,
      0.446201392954695,
      0.46081393728713077,
      0.4294219615752112,
      0.45820181635682455,
      0.4654724085223889,
      0.450982205096833,
      0.4413101544280252,
      0.489913955134546,
      0.4053269123781227,
      0.4332181459861601,
      0.4473999174055225,
      0.4687570190804447,
      0.42149730902945926,
      0.43746406814295374,
      0.42479916820804514,
      0.4221297942949626,
      0.4369574280109948,
      0.42554229940482957,
      0.46213900485260045,
      0.43187996900367165,
      0.4440564736634671,
      0.4518131874278634,
      0.43218881848329554,
      0.4178596933087903,
      0.4297420916532328,
      0.43046934205793336,
      0.4120061571012714,
      0.43545018520183904,
      0.4380008979471858,
      0.4249099986995766,
      0.4367190070138006,
      0.4172876011111779,
      0.44264457295397797,
      0.46238478439118336,
      0.4378796118462157,
      0.46914801069362433,
      0.4185091237197379,
      0.4244918090943805,
      0.44822784214319583,
      0.41383951283143666,
      0.4126164961985485,
      0.39024950255176977,
      0.4168807423221851,
      0.3970379916315307,
      0.4242203257576434,
      0.41348905254623847,
      0.44899455684566214,
      0.4209286803792337,
      0.40990848253051676,
      0.4264246884755746,
      0.42485626182870234,
      0.4059464545574731,
      0.40263060506232484,
      0.40218078196048734,
      0.4183210468310082,
      0.4127632524260504,
      0.46846695520920667,
      0.40177162474322464,
      0.4169119488693283,
      0.4387632034139005,
      0.4723003254143778,
      0.409965937830017,
      0.42134727482310314,
      0.43720894866360877,
      0.4022819982228165,
      0.4178720356907673,
      0.3833128954835994,
      0.3869750703701716,
      0.40810664173014866,
      0.38481755306620796,
      0.40267393000111606,
      0.40731874563379916,
      0.4256565915610262,
      0.4090068428280825,
      0.39311284272970554,
      0.44676296585155817,
      0.40746825007264487,
      0.4135126091049103,
      0.3701618027544307,
      0.42794256283494525,
      0.43008818305181173,
      0.3985169963030044,
      0.4410326224244283,
      0.4113249223656997,
      0.38870535612463236,
      0.3720666656326391,
      0.41198274972374566,
      0.39904527140413215,
      0.3867814326179242,
      0.4065954321337317,
      0.4437415999031352,
      0.4079642819608757,
      0.39049438213159937,
      0.4012353134547879,
      0.43418948835063126,
      0.4267302212690165,
      0.3976194581228816,
      0.4251626498595683,
      0.376883992989977,
      0.40930288564302253,
      0.41342224962340146,
      0.3829663397666223,
      0.3965393988493674,
      0.4100531943454714,
      0.42709464373702777,
      0.4204182042511637,
      0.4120783719652427,
      0.4253880745636489,
      0.42009433778281696,
      0.37107777056579816,
      0.4200344822542396,
      0.40747555708456895,
      0.40804589876157793,
      0.39437801580050746,
      0.40077724729826353,
      0.416166088868401,
      0.41035342280736226,
      0.395425229167153,
      0.41456239027177505,
      0.38094495074477736,
      0.4433314585043285,
      0.40576022170974824,
      0.38448898703038337,
      0.42944817027288995,
      0.3991519467827089,
      0.38924492356662976,
      0.374220598886113,
      0.405438581885335,
      0.40952548133042044
    ],
    "best_epoch": 100,
    "best_val_loss": 0.3701618027544307,
    "test_loss": 0.47057791209320704,
    "tracker": {
      "initial_train_loss": 0.38354347178329257,
      "train_threshold": 0.1278478239277642,
      "best_tracking": true,
      "patience_active": true,
      "patience_best_train": null,
      "patience_best_val": 0.3701618027544307,
      "patience_no_improve_epochs": 50,
      "best_train_loss": Infinity,
      "forced_stop_due_to_threshold": false
    }
  },
  "model_files": {
    "best": "scripts/outputs/mlp-other-value/trial_61_19ac5b/best_model.pt",
    "last": "scripts/outputs/mlp-other-value/trial_61_19ac5b/last_model.pt"
  },
  "config_path": "/home/amoris/data-analysis-on-dbps/scripts/outputs/mlp-other-value/trial_61_19ac5b/config.yaml"
}