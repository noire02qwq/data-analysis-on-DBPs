epoch,train_loss,val_loss
1,0.463013,0.395816
2,0.165324,0.237196
3,0.152054,0.208097
4,0.141453,0.222208
5,0.133333,0.395466
6,0.123200,0.530513
7,0.131634,0.251715
8,0.118192,0.291042
9,0.121845,0.274903
10,0.115758,0.272630
11,0.115405,0.381745
12,0.128332,0.259865
13,0.123316,0.273488
14,0.120283,0.300880
15,0.126809,0.277028
16,0.128015,0.263598
17,0.113896,0.279997
18,0.116066,0.277741
19,0.113359,0.277346
20,0.113669,0.352834
21,0.116719,0.272986
22,0.114682,1.040609
23,0.161452,0.267227
24,0.120886,0.254454
25,0.112924,0.259962
26,0.115962,0.282597
27,0.109733,0.265785
28,0.114904,0.261280
29,0.128935,0.256963
30,0.116097,0.287625
31,0.109316,0.352412
32,0.106433,0.250851
33,0.108506,0.322915
34,0.110014,0.275600
35,0.111341,0.267732
36,0.111796,0.250892
37,0.108800,0.240617
38,0.112216,0.294810
39,0.106949,0.292558
40,0.106380,0.283297
41,0.106889,0.587606
42,0.123622,0.297321
43,0.110436,0.283811
44,0.107222,0.342658
45,0.111477,0.291317
46,0.108752,0.333895
47,0.104514,0.295395
48,0.107747,0.274792
49,0.107265,0.332339
50,0.105472,0.326411
51,0.101795,0.315482
52,0.104969,0.256058
53,0.115926,0.366409
54,0.110908,0.272777
55,0.105781,0.297209
56,0.105131,0.362644
57,0.116908,0.362413
58,0.104106,0.326662
59,0.109458,0.252764
60,0.104743,0.297061
61,0.102160,0.363786
62,0.105926,0.327446
63,0.117628,0.228574
64,0.105962,0.324415
65,0.098950,0.302498
66,0.100068,0.251629
67,0.101763,0.289080
68,0.101083,0.307447
69,0.098290,0.265558
70,0.120306,0.267649
71,0.112104,0.268103
72,0.100824,0.446568
73,0.100178,0.352992
74,0.097030,0.267091
75,0.096173,0.417144
76,0.099744,0.333408
77,0.104396,0.290985
78,0.100869,0.272105
79,0.099770,0.298960
80,0.098307,0.317486
