{
  "model_name": "mlp-other-rate/trial_53_5c212c",
  "model_type": "MLP",
  "model_format": "torch",
  "model_params": {
    "history_length": 1,
    "hidden_layers": [
      512,
      256,
      128
    ],
    "dropout": 0.4995211993296164,
    "mid_layer_count": 15,
    "mid_layer_size": 168
  },
  "training_params": {
    "max_epochs": 400,
    "batch_size": 154,
    "learning_rate": 0.0006869722988460955,
    "weight_decay": 0.0003911017408487959,
    "checkpoint_interval": 10,
    "seed": 42
  },
  "data_csv": "data/time_aligned_data.csv",
  "timestamp_column": "Date, Time",
  "columns": [
    "TRC-DT",
    "TRC-RT",
    "TRC-PPL1",
    "TRC-PPL2",
    "pH-DT",
    "pH-RT",
    "pH-PPL1",
    "pH-PPL2",
    "cond-DT",
    "cond-PPL1",
    "cond-PPL2",
    "fDOM-RT",
    "fDOM-PPL1",
    "fDOM-PPL2",
    "DO-RT",
    "DO-PPL1",
    "DO-PPL2",
    "TOC-RT",
    "TOC-PPL1",
    "TOC-PPL2",
    "DOC-RT",
    "DOC-PPL1",
    "DOC-PPL2",
    "minutes_since_start"
  ],
  "feature_columns": [
    "TRC-DT",
    "TRC-RT",
    "pH-DT",
    "pH-RT",
    "cond-DT",
    "fDOM-RT",
    "DO-RT",
    "TOC-RT",
    "DOC-RT",
    "minutes_since_start"
  ],
  "target_columns": [
    "TOC-PPL1",
    "TOC-PPL2",
    "DOC-PPL1",
    "DOC-PPL2",
    "pH-PPL1",
    "pH-PPL2"
  ],
  "target_mode": "rate",
  "target_base_columns": [
    "TOC-RT",
    "TOC-RT",
    "DOC-RT",
    "DOC-RT",
    "pH-RT",
    "pH-RT"
  ],
  "split_boundaries": {
    "train_end": 7796,
    "val_end": 9466,
    "test_end": 11138
  },
  "dataset_sizes": {
    "train": 7796,
    "val": 1670,
    "test": 1672
  },
  "input_dim": 10,
  "sequence_length": null,
  "training_history": {
    "epochs": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46,
      47,
      48,
      49,
      50,
      51,
      52,
      53,
      54,
      55,
      56,
      57,
      58,
      59,
      60,
      61,
      62,
      63,
      64,
      65,
      66,
      67,
      68,
      69,
      70,
      71,
      72,
      73,
      74,
      75,
      76,
      77,
      78,
      79,
      80,
      81,
      82,
      83,
      84,
      85,
      86,
      87,
      88,
      89,
      90,
      91,
      92,
      93,
      94,
      95,
      96,
      97,
      98,
      99,
      100,
      101,
      102,
      103,
      104,
      105,
      106,
      107,
      108,
      109,
      110,
      111,
      112,
      113,
      114,
      115,
      116,
      117,
      118,
      119,
      120,
      121,
      122
    ],
    "train_loss": [
      0.4798082028094654,
      0.32611892336817994,
      0.2916475741862639,
      0.2735056412019688,
      0.26406292181525126,
      0.25392650260856175,
      0.24498417918852747,
      0.2393974709152992,
      0.23537945264941793,
      0.23023142982914607,
      0.22470314150485948,
      0.22440299621495055,
      0.2212028089794151,
      0.21287597004054692,
      0.21030873689836818,
      0.20773013859314451,
      0.20474069641607734,
      0.20054137395200758,
      0.199654311962376,
      0.1992127387347192,
      0.1934859186961383,
      0.19477473269706508,
      0.1877233587270397,
      0.1889654551792781,
      0.18622813899052823,
      0.18591405996807175,
      0.18251392884842244,
      0.181388956669999,
      0.1794376771317928,
      0.17667428299731142,
      0.17878783558838915,
      0.17595402098591967,
      0.1751767395206633,
      0.17088492757513193,
      0.17050496697150602,
      0.17048038737614257,
      0.1716220922397307,
      0.16937444214945638,
      0.1683819289617871,
      0.16794571613060505,
      0.16756779806790686,
      0.16440967119094835,
      0.16541728740505465,
      0.1622808362778301,
      0.16136129499399215,
      0.16152805284529603,
      0.16067338542962087,
      0.1606990620188067,
      0.15842677667744653,
      0.1581902064195714,
      0.15785241048647355,
      0.15615219661269267,
      0.15492386768465471,
      0.1579899091088142,
      0.15590534931103225,
      0.15508271615785962,
      0.15132564675156554,
      0.15208344825143505,
      0.15086306969880323,
      0.15129557808143412,
      0.15085969833593113,
      0.1500271319150313,
      0.15366417301838797,
      0.15174880942827682,
      0.14932729924845964,
      0.14897829819262304,
      0.14811313872714418,
      0.1474751461184502,
      0.14900459316661874,
      0.14885273870518417,
      0.1467638827818305,
      0.14636962088240668,
      0.14729565676739426,
      0.14639181833907297,
      0.145477406070852,
      0.14269960886311875,
      0.1456026579246208,
      0.14477267634675955,
      0.14473148322773627,
      0.1427929608706574,
      0.14127285809869214,
      0.14383885470252883,
      0.14302179321317932,
      0.1433656552855237,
      0.14299116940216078,
      0.14048118623458097,
      0.13982229866374143,
      0.14144817676649393,
      0.13895705612180415,
      0.13953700464977675,
      0.14132687293868484,
      0.13991052926106598,
      0.13764742693613896,
      0.1381967462056444,
      0.13914370626830455,
      0.13945718370511814,
      0.13948406043222097,
      0.13684551028669523,
      0.13700610369237465,
      0.13779629464559887,
      0.13819584607879218,
      0.13753215305166958,
      0.1368899054255713,
      0.13603024536016478,
      0.13652354334908734,
      0.1379679335542157,
      0.13524275162988836,
      0.13609166484824997,
      0.13767433036147417,
      0.13719172399011315,
      0.13765881247646444,
      0.13646537428095806,
      0.1325334726621121,
      0.1331920369584142,
      0.13437104758052962,
      0.13265969706321754,
      0.13286932779733127,
      0.13471338508020858,
      0.1333397574084364,
      0.13324472006351107,
      0.12998486250799518,
      0.13509505837314736
    ],
    "val_loss": [
      0.3914755417647476,
      0.3748055714065443,
      0.42262650837976773,
      0.4383039289605832,
      0.426640768929156,
      0.47182310733966487,
      0.4956298555978044,
      0.49111571599266485,
      0.48416026149859687,
      0.49453090678253575,
      0.4796352581974275,
      0.4746398477586444,
      0.48798187744474697,
      0.47699893068767596,
      0.4872136774891151,
      0.48867691794555346,
      0.4778204851343246,
      0.48418325406170176,
      0.5108309286975575,
      0.48001903009985736,
      0.4617479919138069,
      0.441392357825876,
      0.49021949389737524,
      0.4687715154386566,
      0.4360795019451016,
      0.45808605100223404,
      0.45722601679627767,
      0.4725150220051497,
      0.478224411464023,
      0.4447887562884542,
      0.44238645820739025,
      0.4500661052094248,
      0.5072484706065612,
      0.44063748427315386,
      0.46803521178439705,
      0.4726972379727278,
      0.45570475373439445,
      0.44165603654113356,
      0.47068591162473145,
      0.47924627518047114,
      0.4551419374114739,
      0.46614834236170716,
      0.4420241150491966,
      0.46680990234463515,
      0.4682131087976302,
      0.4581753012127505,
      0.44785087417342706,
      0.46737874687431813,
      0.4622556429780172,
      0.4458417126072381,
      0.4518973952995803,
      0.4227950403254903,
      0.4118560195504548,
      0.4825065148626259,
      0.4520239721872136,
      0.45195528780985733,
      0.4713816216248952,
      0.4383930293207397,
      0.4460672650686995,
      0.4443536252140285,
      0.4173182750176527,
      0.4396206705156201,
      0.4425028200813396,
      0.4432181771911547,
      0.456695961024233,
      0.4382936127171545,
      0.4420251635555736,
      0.4415832459391234,
      0.44542680912745924,
      0.4577237835008941,
      0.4349096962791717,
      0.4101278337175975,
      0.43175282211539273,
      0.4456338407215244,
      0.44807482079831423,
      0.4404159542507754,
      0.45349922199806053,
      0.43770176636244723,
      0.4515362431546171,
      0.4435309639412486,
      0.45096732314653737,
      0.4543608690004149,
      0.44010027861166856,
      0.43364226457601535,
      0.4535981151515138,
      0.4337757154317673,
      0.424903145807232,
      0.4532546059992499,
      0.42911488633312866,
      0.45904979438125015,
      0.4648162624346996,
      0.4321001515773956,
      0.4529493634750743,
      0.45075516163588997,
      0.45312072092187616,
      0.44655835906902475,
      0.45579755554299156,
      0.4423831749818996,
      0.41783945887923957,
      0.45840886730455355,
      0.4494961896788574,
      0.4370311040899711,
      0.4356790666094797,
      0.4101803791737128,
      0.41421803110195493,
      0.430829245994191,
      0.431062003863072,
      0.4182169275369473,
      0.4235712408216414,
      0.4286948858442421,
      0.44441794119552225,
      0.441181740450288,
      0.4448788067537867,
      0.44060452554575696,
      0.44833873780723105,
      0.4166143585018769,
      0.44649445062982823,
      0.431351590049481,
      0.4433812447739932,
      0.4358340989686772,
      0.4170741769992663,
      0.4338736874079276
    ],
    "best_epoch": 72,
    "best_val_loss": 0.4101278337175975,
    "best_val_abs_mse": 0.4985329210758209,
    "test_loss": 0.5102569192255798,
    "test_abs_mse": 1.143826961517334,
    "tracker": {
      "initial_train_loss": 0.4798082028094654,
      "train_threshold": 0.15993606760315512,
      "best_tracking": true,
      "patience_active": true,
      "patience_best_train": null,
      "patience_best_val": 0.4101278337175975,
      "patience_no_improve_epochs": 50,
      "best_train_loss": Infinity,
      "forced_stop_due_to_threshold": false
    }
  },
  "model_files": {
    "best": "scripts/outputs/mlp-other-rate/trial_53_5c212c/best_model.pt",
    "last": "scripts/outputs/mlp-other-rate/trial_53_5c212c/last_model.pt"
  },
  "config_path": "/home/amoris/data-analysis-on-dbps/scripts/outputs/mlp-other-rate/trial_53_5c212c/config.yaml"
}