{
  "model_name": "mlp-other-value/trial_22_442391",
  "model_type": "MLP",
  "model_format": "torch",
  "model_params": {
    "history_length": 1,
    "hidden_layers": [
      512,
      256,
      128
    ],
    "dropout": 0.26848363483276644,
    "mid_layer_count": 3,
    "mid_layer_size": 448
  },
  "training_params": {
    "max_epochs": 400,
    "batch_size": 178,
    "learning_rate": 0.0016212255402406448,
    "weight_decay": 0.00018942150996250108,
    "checkpoint_interval": 10,
    "seed": 42
  },
  "data_csv": "data/time_aligned_data.csv",
  "timestamp_column": "Date, Time",
  "columns": [
    "TRC-DT",
    "TRC-RT",
    "TRC-PPL1",
    "TRC-PPL2",
    "pH-DT",
    "pH-RT",
    "pH-PPL1",
    "pH-PPL2",
    "cond-DT",
    "cond-PPL1",
    "cond-PPL2",
    "fDOM-RT",
    "fDOM-PPL1",
    "fDOM-PPL2",
    "DO-RT",
    "DO-PPL1",
    "DO-PPL2",
    "TOC-RT",
    "TOC-PPL1",
    "TOC-PPL2",
    "DOC-RT",
    "DOC-PPL1",
    "DOC-PPL2",
    "minutes_since_start"
  ],
  "feature_columns": [
    "TRC-DT",
    "TRC-RT",
    "pH-DT",
    "pH-RT",
    "cond-DT",
    "fDOM-RT",
    "DO-RT",
    "TOC-RT",
    "DOC-RT",
    "minutes_since_start"
  ],
  "target_columns": [
    "TOC-PPL1",
    "TOC-PPL2",
    "DOC-PPL1",
    "DOC-PPL2",
    "pH-PPL1",
    "pH-PPL2"
  ],
  "split_boundaries": {
    "train_end": 7796,
    "val_end": 9466,
    "test_end": 11138
  },
  "dataset_sizes": {
    "train": 7796,
    "val": 1670,
    "test": 1672
  },
  "input_dim": 10,
  "sequence_length": null,
  "training_history": {
    "epochs": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44,
      45,
      46,
      47,
      48,
      49,
      50,
      51,
      52,
      53,
      54,
      55,
      56,
      57,
      58,
      59,
      60,
      61,
      62,
      63,
      64,
      65,
      66,
      67,
      68,
      69,
      70,
      71,
      72,
      73,
      74,
      75,
      76,
      77,
      78,
      79,
      80,
      81,
      82,
      83,
      84,
      85,
      86,
      87,
      88,
      89,
      90,
      91,
      92,
      93,
      94,
      95,
      96,
      97,
      98,
      99,
      100,
      101,
      102,
      103,
      104,
      105,
      106,
      107,
      108,
      109,
      110,
      111,
      112,
      113,
      114,
      115,
      116,
      117,
      118
    ],
    "train_loss": [
      0.35568183468221826,
      0.2364239867541532,
      0.21374941118378832,
      0.19662159604100218,
      0.1866829831434128,
      0.1765516092489474,
      0.1680629593422928,
      0.16327975670486428,
      0.15536399003502466,
      0.15285073718454606,
      0.1479968405235511,
      0.14604680592996025,
      0.14160352931374645,
      0.13611285864786957,
      0.13391290399976544,
      0.133103142859937,
      0.128923680040777,
      0.1254186762593354,
      0.1253777170212621,
      0.12439035839487676,
      0.12200571285831188,
      0.1182200608229319,
      0.11956300880433107,
      0.1184036503557886,
      0.11462071279743319,
      0.11222382112878784,
      0.10956382973185559,
      0.11168031666280674,
      0.109595686787899,
      0.10665515661736584,
      0.10698329216932993,
      0.1050551421519822,
      0.10624800861577549,
      0.10486115149584044,
      0.10233605714211896,
      0.10154946991316535,
      0.10031735134039126,
      0.10188433179118622,
      0.1004123822745297,
      0.09939517762383905,
      0.09933607678969375,
      0.0997318186118182,
      0.09753271378098909,
      0.0966638665113038,
      0.09527879526954298,
      0.09502513543260654,
      0.09452274243508381,
      0.09629297888564904,
      0.09581556301344168,
      0.0946427471668155,
      0.09587736911173354,
      0.09321097441289045,
      0.09320676502025757,
      0.09334601957318475,
      0.09277847718128246,
      0.09269934489352083,
      0.09229345089651304,
      0.09350799278319341,
      0.09124356171086606,
      0.09057345244939845,
      0.08976186223026114,
      0.08778562778468435,
      0.09028101473542101,
      0.08971134776210345,
      0.08881481736133869,
      0.08868634773407795,
      0.08916063900327854,
      0.08956194306036153,
      0.08812654908916594,
      0.08771832232263652,
      0.0866652200521262,
      0.08949763665578804,
      0.08823821365282804,
      0.08630543499764508,
      0.08397587302741544,
      0.08522853421180171,
      0.08626861718524842,
      0.08655316345183406,
      0.08501454211652983,
      0.08531104011404753,
      0.08477492850482189,
      0.08573985878299786,
      0.08586766071361478,
      0.08537077941592991,
      0.0852216926349347,
      0.08514837619049112,
      0.08528815409605353,
      0.0854524778075069,
      0.08334011214476111,
      0.08484043854323639,
      0.08326559826892238,
      0.08483918692788568,
      0.08209086755045014,
      0.08149183016478741,
      0.08225374909288886,
      0.08251661364812922,
      0.08518318566238042,
      0.08164045828781169,
      0.08303999586586099,
      0.08416461703554369,
      0.0823321835242552,
      0.08188430429567674,
      0.08275985078032717,
      0.08114011914853013,
      0.08157396311940995,
      0.08183620678592547,
      0.08402537011286734,
      0.08364589641366695,
      0.08111184840403562,
      0.08061850091405194,
      0.08138042227916989,
      0.08239408418260946,
      0.08168666928685464,
      0.0819468536973764,
      0.0811482823987598,
      0.08130384153046627,
      0.08045227748995075,
      0.08036809729812633
    ],
    "val_loss": [
      0.40802753203643294,
      0.5743525027961074,
      0.5253496389010709,
      0.5277595055852822,
      0.5250960180502452,
      0.5736180750374309,
      0.5267392487761503,
      0.5731579902286301,
      0.5744718808256938,
      0.6248615037984477,
      0.6831479784614312,
      0.5575735676877512,
      0.479580418578165,
      0.5344857662291584,
      0.5069324451827717,
      0.47329885281488565,
      0.4378103321630084,
      0.5176656321315708,
      0.6996250801992987,
      0.4018204158740843,
      0.4261105370111094,
      0.41596324136871066,
      0.49585569269821317,
      0.4276472436989139,
      0.47550010809641396,
      0.47811343354022434,
      0.4952244292119306,
      0.49431302545670264,
      0.4684550432031026,
      0.44975887287162736,
      0.4398011951567884,
      0.45906463837373757,
      0.5297595195427626,
      0.4760392955498781,
      0.4102899984685247,
      0.425896933692658,
      0.4470497208441089,
      0.4356680234481475,
      0.42682282598075755,
      0.47454367669935,
      0.49845532375181506,
      0.465796454224044,
      0.457057219600963,
      0.4580249966172401,
      0.4272186802800544,
      0.4637709117042804,
      0.4741582905550203,
      0.4144707797887083,
      0.4333096212344969,
      0.45161013341770917,
      0.4119285757045546,
      0.3894143415068438,
      0.3810822172079258,
      0.41608557804615914,
      0.4194565859651137,
      0.43463380028923115,
      0.4082688500602802,
      0.41811940189071756,
      0.44395095114401,
      0.42221417538598627,
      0.43931719725895785,
      0.42676895892548705,
      0.38395581957465874,
      0.438437165953442,
      0.38488132377227624,
      0.4249659749026784,
      0.4401293852550541,
      0.3714288359362922,
      0.42240868442786667,
      0.4170938120809144,
      0.44726217240987426,
      0.3937116897391702,
      0.42157014135472076,
      0.4188260916107429,
      0.4405082445747838,
      0.43055998062837625,
      0.40654312927208974,
      0.40304883639969513,
      0.38040790177747874,
      0.40885714489721253,
      0.4372343453283081,
      0.3946801631036633,
      0.3727595383000231,
      0.41832710079804153,
      0.4434692400836659,
      0.4180500214149852,
      0.416600884595317,
      0.43029854241423976,
      0.43883193443099894,
      0.40777490261250626,
      0.42349161622588505,
      0.3842246595167828,
      0.4320470019282695,
      0.42265072146397153,
      0.41379626622635446,
      0.4403407726905303,
      0.38530088375785393,
      0.43789006841039946,
      0.44253693971626773,
      0.4095211113105991,
      0.4398025966511515,
      0.41482283330249214,
      0.4375415663608534,
      0.4467450892496966,
      0.4723876828205086,
      0.450372845023692,
      0.4051381997897953,
      0.4433630480380829,
      0.44510346090722225,
      0.5565768840962542,
      0.43189601766135166,
      0.40087844498321684,
      0.47207625162994077,
      0.46957263306764785,
      0.5062808981912579,
      0.44275722413541313,
      0.4812803386392708,
      0.5373433658462798
    ],
    "best_epoch": 68,
    "best_val_loss": 0.3714288359362922,
    "test_loss": 0.4581671730842459,
    "tracker": {
      "initial_train_loss": 0.35568183468221826,
      "train_threshold": 0.11856061156073942,
      "best_tracking": true,
      "patience_active": true,
      "patience_best_train": null,
      "patience_best_val": 0.3714288359362922,
      "patience_no_improve_epochs": 50,
      "best_train_loss": Infinity,
      "forced_stop_due_to_threshold": false
    }
  },
  "model_files": {
    "best": "scripts/outputs/mlp-other-value/trial_22_442391/best_model.pt",
    "last": "scripts/outputs/mlp-other-value/trial_22_442391/last_model.pt"
  },
  "config_path": "/home/amoris/data-analysis-on-dbps/scripts/outputs/mlp-other-value/trial_22_442391/config.yaml"
}